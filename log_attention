Dataframe creation done
0 category count=27
training loss: 0.650497555732727
training loss: 0.6304605007171631
training loss: 0.5400671362876892
training loss: 0.46228307485580444
training loss: 0.327637255191803
training loss: 0.307466596364975
training loss: 0.2908385992050171
training loss: 0.4554295241832733
training loss: 0.22197872400283813
training loss: 0.22418318688869476
training loss: 0.6284628510475159
training loss: 0.24228936433792114
training loss: 0.19235394895076752
training loss: 0.1619642823934555
training loss: 0.1801707148551941
training loss: 0.1841508001089096
training loss: 0.16640830039978027
training loss: 0.17544028162956238
training loss: 0.3861599862575531
training loss: 0.1516023576259613
training loss: 0.1804085224866867
training loss: 0.17007923126220703
training loss: 0.17770783603191376
training loss: 0.16669005155563354
training loss: 0.1469039022922516
training loss: 0.3985978066921234
training loss: 0.5980422496795654
training loss: 0.39383384585380554
training loss: 0.41560807824134827
training loss: 0.15316154062747955
training loss: 0.14322605729103088
training loss: 0.1421840935945511
training loss: 0.1518508791923523
training loss: 0.139630526304245
training loss: 0.1506771296262741
training loss: 0.15333718061447144
training loss: 0.41613903641700745
training loss: 0.14707203209400177
training loss: 0.13017573952674866
training loss: 0.15341304242610931
training loss: 0.157798171043396
training loss: 0.16170382499694824
training loss: 0.1363229602575302
training loss: 0.24450474977493286
training loss: 0.3782721161842346
training loss: 0.3784387707710266
training loss: 0.15785305202007294
training loss: 0.373196542263031
training loss: 0.14786487817764282
training loss: 0.14087185263633728
training loss: 0.13462357223033905
training loss: 0.3797401785850525
training loss: 0.3852367699146271
training loss: 0.14043819904327393
training loss: 0.14746665954589844
training loss: 0.39942607283592224
training loss: 0.13987810909748077
training loss: 0.3878079056739807
training loss: 0.13923920691013336
training loss: 0.15681084990501404
training loss: 0.14221300184726715
training loss: 0.1354667991399765
training loss: 0.1367117166519165
training loss: 0.1365174651145935
training loss: 0.1532142162322998
training loss: 0.146209254860878
training loss: 0.13884732127189636
training loss: 0.13555380702018738
training loss: 0.13906824588775635
training loss: 0.1312595009803772
training loss: 0.13292284309864044
training loss: 0.1466473788022995
training loss: 0.3621913194656372
training loss: 0.14050045609474182
training loss: 0.1375073492527008
training loss: 0.6296513080596924
training loss: 0.13248193264007568
training loss: 0.3648698031902313
training loss: 0.3598232567310333
training loss: 0.13225294649600983
training loss: 0.13825462758541107
training loss: 0.13501279056072235
training loss: 0.1420965939760208
training loss: 0.13138934969902039
training loss: 0.13775138556957245
training loss: 0.15116329491138458
training loss: 0.6235565543174744
training loss: 0.1303027719259262
training loss: 0.12968887388706207
training loss: 0.13407164812088013
training loss: 0.3659127652645111
training loss: 0.13976922631263733
training loss: 0.38621973991394043
training loss: 0.14020539820194244
training loss: 0.3737964630126953
training loss: 0.1330270767211914
training loss: 0.12863585352897644
training loss: 0.13544447720050812
training loss: 0.13355673849582672
training loss: 0.14098680019378662
training loss: 0.14105753600597382
training loss: 0.13957542181015015
training loss: 0.14053209125995636
training loss: 0.13772930204868317
training loss: 0.13438107073307037
training loss: 0.1314232051372528
training loss: 0.13559575378894806
training loss: 0.13739146292209625
training loss: 0.3769756257534027
training loss: 0.13132573664188385
training loss: 0.3648379445075989
training loss: 0.13369552791118622
training loss: 0.13282637298107147
training loss: 0.13008932769298553
training loss: 0.1289791464805603
training loss: 0.13024337589740753
training loss: 0.1304161697626114
training loss: 0.13265888392925262
training loss: 0.13347668945789337
training loss: 0.1321868747472763
training loss: 0.3757328987121582
training loss: 0.375725120306015
training loss: 0.12769758701324463
training loss: 0.13188593089580536
training loss: 0.3777279853820801
training loss: 0.13232456147670746
training loss: 0.13269512355327606
training loss: 0.14066484570503235
training loss: 0.1379651427268982
training loss: 0.12981131672859192
training loss: 0.138032466173172
training loss: 0.12956492602825165
training loss: 0.14220614731311798
training loss: 0.38479533791542053
training loss: 0.38655421137809753
training loss: 0.13029932975769043
training loss: 0.15417616069316864
training loss: 0.13859809935092926
training loss: 0.12836198508739471
training loss: 0.37638989090919495
training loss: 0.1298079490661621
training loss: 0.12782596051692963
training loss: 0.37371036410331726
training loss: 0.12862466275691986
training loss: 0.40499240159988403
training loss: 0.38000965118408203
training loss: 0.1311558485031128
training loss: 0.1297188699245453
training loss: 0.13098974525928497
training loss: 0.12801866233348846
training loss: 0.13368044793605804
training loss: 0.12778279185295105
training loss: 0.12881362438201904
training loss: 0.12872065603733063
training loss: 0.12775814533233643
training loss: 0.1276094615459442
training loss: 0.3830239176750183
training loss: 0.6261979341506958
training loss: 0.12895351648330688
training loss: 0.13422781229019165
training loss: 0.1441289782524109
training loss: 0.12804213166236877
training loss: 0.1295042783021927
training loss: 0.36927929520606995
training loss: 0.3792511522769928
training loss: 0.37824639678001404
training loss: 0.13168206810951233
training loss: 0.630219578742981
training loss: 0.37678658962249756
training loss: 0.12847711145877838
training loss: 0.12825947999954224
training loss: 0.3799687623977661
training loss: 0.1356097012758255
training loss: 0.12933555245399475
training loss: 0.37949156761169434
training loss: 0.12802386283874512
training loss: 0.12837839126586914
training loss: 0.13251106441020966
training loss: 0.1288316696882248
training loss: 0.12901899218559265
training loss: 0.12783916294574738
training loss: 0.12986642122268677
training loss: 0.13081364333629608
training loss: 0.37191247940063477
training loss: 0.6248984932899475
training loss: 0.1354326456785202
training loss: 0.12839581072330475
training loss: 0.1297122687101364
training loss: 0.12970229983329773
training loss: 0.1331823766231537
training loss: 0.12880288064479828
training loss: 0.13997510075569153
training loss: 0.1280285269021988
training loss: 0.12753494083881378
training loss: 0.3779858946800232
training loss: 0.13114964962005615
training loss: 0.37094467878341675
training loss: 0.12867596745491028
training loss: 0.12822629511356354
training loss: 0.12749844789505005
training loss: 0.12840361893177032
training loss: 0.1293237805366516
training loss: 0.37532976269721985
training loss: 0.12722578644752502
training loss: 0.12879686057567596
training loss: 0.12791962921619415
training loss: 0.1285184621810913
training loss: 0.1288931667804718
training loss: 0.6287297010421753
training loss: 0.12962466478347778
training loss: 0.37716418504714966
training loss: 0.13180474936962128
training loss: 0.13143889605998993
training loss: 0.12786069512367249
training loss: 0.1293153613805771
training loss: 0.12721188366413116
training loss: 0.12923884391784668
training loss: 0.37801656126976013
training loss: 0.12875896692276
training loss: 0.1273735612630844
training loss: 0.12882137298583984
training loss: 0.37852925062179565
training loss: 0.1303815245628357
training loss: 0.1279200166463852
training loss: 0.12756730616092682
training loss: 0.12778688967227936
training loss: 0.1324891299009323
training loss: 0.13078457117080688
training loss: 0.3766838610172272
training loss: 0.626735270023346
training loss: 0.12834982573986053
training loss: 0.12936027348041534
training loss: 0.12756958603858948
training loss: 0.37286069989204407
training loss: 0.13002952933311462
training loss: 0.37628087401390076
training loss: 0.12846727669239044
training loss: 0.12734359502792358
training loss: 0.1306217461824417
training loss: 0.12871702015399933
training loss: 0.13019855320453644
training loss: 0.13297522068023682
training loss: 0.3770033121109009
training loss: 0.12799198925495148
training loss: 0.12902717292308807
training loss: 0.12764863669872284
training loss: 0.1281900852918625
training loss: 0.1302371323108673
training loss: 0.1289484202861786
training loss: 0.12953555583953857
training loss: 0.12783534824848175
training loss: 0.12945085763931274
training loss: 0.13957983255386353
training loss: 0.12953506410121918
training loss: 0.12737013399600983
training loss: 0.37815040349960327
training loss: 0.12865875661373138
training loss: 0.13000769913196564
training loss: 0.12952855229377747
training loss: 0.127348855137825
training loss: 0.3775019347667694
training loss: 0.37793809175491333
training loss: 0.623569667339325
training loss: 0.12997373938560486
training loss: 0.12902364134788513
training loss: 0.3812848627567291
training loss: 0.12765617668628693
training loss: 0.12713024020195007
training loss: 0.12835538387298584
training loss: 0.6231627464294434
training loss: 0.1273604780435562
training loss: 0.130043163895607
training loss: 0.1282484233379364
training loss: 0.13014045357704163
training loss: 0.3798605799674988
training loss: 0.12866821885108948
training loss: 0.12775206565856934
training loss: 0.1272212713956833
training loss: 0.3744954764842987
training loss: 0.13404439389705658
training loss: 0.13026829063892365
training loss: 0.13010461628437042
training loss: 0.13002055883407593
training loss: 0.1304570883512497
training loss: 0.12752920389175415
training loss: 0.12733407318592072
training loss: 0.37733301520347595
training loss: 0.13613010942935944
training loss: 0.12943178415298462
training loss: 0.13163040578365326
training loss: 0.3703269064426422
training loss: 0.12717470526695251
training loss: 0.37743470072746277
training loss: 0.12831106781959534
training loss: 0.12715768814086914
training loss: 0.13148647546768188
training loss: 0.3782491683959961
training loss: 0.12817835807800293
training loss: 0.12762950360774994
training loss: 0.12914584577083588
training loss: 0.13290004432201385
training loss: 0.1280944049358368
training loss: 0.12775184214115143
training loss: 0.12978462874889374
training loss: 0.12780563533306122
training loss: 0.12740901112556458
training loss: 0.37608709931373596
training loss: 0.12780840694904327
training loss: 0.1281091272830963
training loss: 0.1302815079689026
training loss: 0.12854363024234772
training loss: 0.1316661387681961
training loss: 0.13044311106204987
training loss: 0.12816646695137024
training loss: 0.1282201111316681
training loss: 0.37695416808128357
training loss: 0.1281307339668274
training loss: 0.12773440778255463
training loss: 0.1278112530708313
training loss: 0.1281195729970932
training loss: 0.12914802134037018
training loss: 0.13168713450431824
training loss: 0.37739428877830505
training loss: 0.3794339895248413
training loss: 0.12789271771907806
training loss: 0.37768980860710144
training loss: 0.6266425251960754
training loss: 0.12959498167037964
training loss: 0.13004186749458313
training loss: 0.13030995428562164
training loss: 0.13423864543437958
training loss: 0.1287284791469574
training loss: 0.6284926533699036
training loss: 0.129570871591568
training loss: 0.37770822644233704
training loss: 0.12834694981575012
training loss: 0.12923911213874817
training loss: 0.12719643115997314
training loss: 0.1288197785615921
training loss: 0.13077005743980408
training loss: 0.12835851311683655
training loss: 0.1283932775259018
training loss: 0.3784180283546448
training loss: 0.1275036334991455
training loss: 0.12751834094524384
training loss: 0.12710322439670563
training loss: 0.12850826978683472
training loss: 0.12731970846652985
training loss: 0.12732921540737152
training loss: 0.12732529640197754
training loss: 0.37861019372940063
training loss: 0.12736469507217407
training loss: 0.37670451402664185
training loss: 0.6264927387237549
training loss: 0.12724338471889496
training loss: 0.13473819196224213
training loss: 0.1276022493839264
training loss: 0.6202512979507446
training loss: 0.12737230956554413
training loss: 0.12733036279678345
training loss: 0.12795521318912506
training loss: 0.1286516934633255
training loss: 0.13569222390651703
training loss: 0.12693330645561218
training loss: 0.12726576626300812
training loss: 0.13342861831188202
training loss: 0.12792757153511047
training loss: 0.3770643174648285
training loss: 0.12720538675785065
training loss: 0.12941302359104156
training loss: 0.12715314328670502
training loss: 0.1506650298833847
training loss: 0.37752556800842285
training loss: 0.1331644058227539
training loss: 0.12808465957641602
training loss: 0.1294417679309845
training loss: 0.12779025733470917
training loss: 0.12849289178848267
training loss: 0.12730176746845245
training loss: 0.1283312439918518
training loss: 0.12796789407730103
training loss: 0.12772968411445618
training loss: 0.12889322638511658
training loss: 0.12743517756462097
training loss: 0.3775673806667328
training loss: 0.6279261708259583
training loss: 0.12755757570266724
training loss: 0.37721264362335205
training loss: 0.12806639075279236
training loss: 0.12710143625736237
training loss: 0.3768472671508789
training loss: 0.13033482432365417
training loss: 0.12750661373138428
training loss: 0.12804511189460754
training loss: 0.3787684142589569
training loss: 0.12796567380428314
training loss: 0.13129213452339172
training loss: 0.12793761491775513
training loss: 0.12733051180839539
training loss: 0.12865184247493744
training loss: 0.1316685974597931
training loss: 0.12714475393295288
training loss: 0.12708337604999542
training loss: 0.12704384326934814
training loss: 0.12745456397533417
training loss: 0.376548707485199
training loss: 0.12732328474521637
training loss: 0.12805067002773285
training loss: 0.12706924974918365
training loss: 0.12805083394050598
training loss: 0.12897484004497528
training loss: 0.37427839636802673
training loss: 0.3769436180591583
training loss: 0.12714873254299164
training loss: 0.13011842966079712
training loss: 0.1270609349012375
training loss: 0.1275005340576172
training loss: 0.37780842185020447
training loss: 0.12742851674556732
training loss: 0.1275339424610138
training loss: 0.37780964374542236
training loss: 0.37626662850379944
training loss: 0.1270400434732437
training loss: 0.12750832736492157
training loss: 0.1394338607788086
training loss: 0.127047598361969
training loss: 0.12724216282367706
training loss: 0.1275252252817154
training loss: 0.1274166852235794
training loss: 0.3749863803386688
training loss: 0.6269587278366089
training loss: 0.12981507182121277
training loss: 0.12702438235282898
training loss: 0.12757377326488495
training loss: 0.12773217260837555
training loss: 0.12739185988903046
training loss: 0.3768794536590576
training loss: 0.12776373326778412
training loss: 0.13153187930583954
training loss: 0.3770730793476105
training loss: 0.12698227167129517
training loss: 0.12713013589382172
training loss: 0.1322489082813263
training loss: 0.1273915320634842
training loss: 0.12763690948486328
training loss: 0.12899400293827057
training loss: 0.12809419631958008
training loss: 0.12714797258377075
training loss: 0.12714916467666626
training loss: 0.12712794542312622
training loss: 0.3772968053817749
training loss: 0.3813917636871338
training loss: 0.12763789296150208
training loss: 0.12719625234603882
training loss: 0.12704432010650635
training loss: 0.3775244355201721
training loss: 0.3772822320461273
training loss: 0.1273772269487381
training loss: 0.12756861746311188
training loss: 0.12783533334732056
training loss: 0.12762750685214996
training loss: 0.37752342224121094
training loss: 0.12707623839378357
training loss: 0.12755128741264343
training loss: 0.12845104932785034
training loss: 0.3772035241127014
training loss: 0.1272762417793274
training loss: 0.6293208599090576
training loss: 0.1272088587284088
training loss: 0.12877601385116577
training loss: 0.12754566967487335
training loss: 0.3799351155757904
training loss: 0.627183735370636
training loss: 0.37749117612838745
training loss: 0.13186566531658173
training loss: 0.37246251106262207
training loss: 0.1270337849855423
training loss: 0.3756067752838135
training loss: 0.12911845743656158
training loss: 0.14048661291599274
training loss: 0.128452330827713
training loss: 0.12714917957782745
training loss: 0.12753479182720184
training loss: 0.12739965319633484
training loss: 0.12964878976345062
training loss: 0.12732015550136566
training loss: 0.3770242929458618
training loss: 0.12782400846481323
training loss: 0.37834084033966064
training loss: 0.12702050805091858
training loss: 0.1276991218328476
training loss: 0.12814736366271973
training loss: 0.12720975279808044
training loss: 0.12706340849399567
training loss: 0.12871554493904114
training loss: 0.1290353387594223
training loss: 0.1272309124469757
training loss: 0.12699498236179352
training loss: 0.12885253131389618
training loss: 0.37743815779685974
training loss: 0.1272761970758438
training loss: 0.37127548456192017
training loss: 0.12922987341880798
training loss: 0.1270466446876526
training loss: 0.12749043107032776
training loss: 0.1275661587715149
training loss: 0.12885957956314087
training loss: 0.3771860897541046
training loss: 0.12723086774349213
training loss: 0.12701179087162018
training loss: 0.12717820703983307
training loss: 0.12710034847259521
training loss: 0.12768486142158508
training loss: 0.12790705263614655
training loss: 0.3751704692840576
training loss: 0.1493588387966156
training loss: 0.12742146849632263
training loss: 0.12993457913398743
training loss: 0.12858979403972626
training loss: 0.1276412010192871
training loss: 0.12771081924438477
training loss: 0.12727227807044983
training loss: 0.12791642546653748
training loss: 0.13737589120864868
training loss: 0.1277293860912323
training loss: 0.12697523832321167
training loss: 0.12759976089000702
training loss: 0.1274959295988083
training loss: 0.12729115784168243
training loss: 0.12722128629684448
training loss: 0.12723562121391296
training loss: 0.12837380170822144
training loss: 0.1282883584499359
training loss: 0.12878862023353577
training loss: 0.12774625420570374
training loss: 0.12806349992752075
training loss: 0.129582017660141
training loss: 0.37688684463500977
training loss: 0.12801149487495422
training loss: 0.12744905054569244
training loss: 0.37751972675323486
training loss: 0.3775479793548584
training loss: 0.12915368378162384
training loss: 0.12725293636322021
training loss: 0.12715420126914978
training loss: 0.12785503268241882
training loss: 0.12706220149993896
training loss: 0.12713395059108734
training loss: 0.12985505163669586
training loss: 0.12718342244625092
training loss: 0.37924206256866455
training loss: 0.1273486614227295
training loss: 0.12698747217655182
training loss: 0.3773294687271118
training loss: 0.12767785787582397
training loss: 0.12696091830730438
training loss: 0.1290346086025238
training loss: 0.12808310985565186
training loss: 0.3784649968147278
training loss: 0.37740883231163025
training loss: 0.12725631892681122
training loss: 0.1285356879234314
training loss: 0.3766101002693176
training loss: 0.1277460753917694
training loss: 0.12859776616096497
training loss: 0.12712134420871735
training loss: 0.1281575709581375
training loss: 0.37186968326568604
training loss: 0.1271481066942215
training loss: 0.3764042854309082
training loss: 0.12728825211524963
training loss: 0.1287052184343338
training loss: 0.8769192099571228
training loss: 0.3777732849121094
training loss: 0.1279272586107254
training loss: 0.12735947966575623
training loss: 0.1269822120666504
training loss: 0.1274811029434204
training loss: 0.1272517740726471
training loss: 0.12739083170890808
training loss: 0.6269223690032959
training loss: 0.1304127424955368
training loss: 0.12734545767307281
training loss: 0.12718141078948975
training loss: 0.12929865717887878
training loss: 0.1272548884153366
training loss: 0.12704916298389435
training loss: 0.12700225412845612
training loss: 0.127801313996315
training loss: 0.12730854749679565
training loss: 0.3773905038833618
training loss: 0.13037098944187164
training loss: 0.12841850519180298
training loss: 0.12694883346557617
training loss: 0.1276327669620514
training loss: 0.12717650830745697
training loss: 0.37584513425827026
training loss: 0.12862108647823334
training loss: 0.12757986783981323
training loss: 0.1271071434020996
training loss: 0.12731920182704926
training loss: 0.12719771265983582
training loss: 0.12719370424747467
training loss: 0.1290227472782135
training loss: 0.12741196155548096
training loss: 0.12721098959445953
training loss: 0.1269969940185547
training loss: 0.6269698143005371
training loss: 0.6265886425971985
training loss: 0.12721623480319977
training loss: 0.1271112561225891
training loss: 0.37757354974746704
training loss: 0.12831975519657135
training loss: 0.12758171558380127
training loss: 0.13260792195796967
training loss: 0.3767598569393158
training loss: 0.12715281546115875
training loss: 0.12782038748264313
training loss: 0.12707026302814484
training loss: 0.12709318101406097
training loss: 0.12715548276901245
training loss: 0.36274147033691406
training loss: 0.1347159445285797
training loss: 0.12736015021800995
training loss: 0.1279088258743286
training loss: 0.12720060348510742
training loss: 0.3771025538444519
training loss: 0.12745380401611328
training loss: 0.12694993615150452
training loss: 0.12721934914588928
training loss: 0.12815634906291962
training loss: 0.1281416416168213
training loss: 0.37693849205970764
training loss: 0.12702153623104095
training loss: 0.1273258626461029
training loss: 0.12737134099006653
training loss: 0.12834323942661285
training loss: 0.37705403566360474
training loss: 0.12737692892551422
training loss: 0.37903738021850586
training loss: 0.1272851675748825
training loss: 0.3769626021385193
training loss: 0.12709060311317444
training loss: 0.1275128275156021
training loss: 0.37726345658302307
training loss: 0.12772847712039948
training loss: 0.12767386436462402
training loss: 0.37790173292160034
training loss: 0.6268478631973267
training loss: 0.12743856012821198
training loss: 0.3771372437477112
training loss: 0.12702538073062897
training loss: 0.13084466755390167
training loss: 0.12809574604034424
training loss: 0.1281270980834961
training loss: 0.3770386874675751
training loss: 0.12872207164764404
training loss: 0.1270059049129486
training loss: 0.1272733360528946
training loss: 0.37890946865081787
training loss: 0.1271498203277588
training loss: 0.1271565556526184
training loss: 0.12718814611434937
training loss: 0.12789560854434967
training loss: 0.12962232530117035
training loss: 0.37644994258880615
training loss: 0.12774209678173065
training loss: 0.12735193967819214
training loss: 0.37698566913604736
training loss: 0.12714752554893494
training loss: 0.37741610407829285
training loss: 0.12707947194576263
training loss: 0.12950493395328522
training loss: 0.12699368596076965
training loss: 0.3769771158695221
training loss: 0.6269950866699219
training loss: 0.1273537576198578
training loss: 0.127665713429451
training loss: 0.1269325613975525
training loss: 0.12761440873146057
training loss: 0.12788915634155273
training loss: 0.12718117237091064
training loss: 0.377280056476593
training loss: 0.37682655453681946
training loss: 0.12698706984519958
training loss: 0.627313494682312
training loss: 0.3772989809513092
training loss: 0.1270420104265213
training loss: 0.1292109340429306
training loss: 0.12707936763763428
training loss: 0.12745365500450134
training loss: 0.37081024050712585
training loss: 0.12700814008712769
training loss: 0.12716680765151978
training loss: 0.12758344411849976
training loss: 0.12770415842533112
training loss: 0.1271360069513321
training loss: 0.12763740122318268
training loss: 0.1278151571750641
training loss: 0.12707170844078064
training loss: 0.12785886228084564
training loss: 0.12850770354270935
training loss: 0.12716037034988403
training loss: 0.3769044280052185
training loss: 0.12698790431022644
training loss: 0.37742263078689575
training loss: 0.1269741803407669
training loss: 0.1279398798942566
training loss: 0.1270197629928589
training loss: 0.37727218866348267
training loss: 0.12976479530334473
training loss: 0.1271359920501709
training loss: 0.3771507740020752
training loss: 0.37695935368537903
training loss: 0.1279219537973404
training loss: 0.1270616054534912
training loss: 0.1271275132894516
training loss: 0.12753239274024963
training loss: 0.3771844506263733
training loss: 0.3758416175842285
training loss: 0.3768197298049927
training loss: 0.3767753839492798
training loss: 0.12697364389896393
training loss: 0.12725207209587097
training loss: 0.12730585038661957
training loss: 0.6275436282157898
training loss: 0.13483715057373047
training loss: 0.12693004310131073
Average loss at epoch 1: 0.19584543360294876
training loss: 0.12728866934776306
training loss: 0.1273372918367386
training loss: 0.12806710600852966
training loss: 0.12708298861980438
training loss: 0.3769415020942688
training loss: 0.12700244784355164
training loss: 0.3770281970500946
training loss: 0.12737657129764557
training loss: 0.12811464071273804
training loss: 0.377155065536499
training loss: 0.12754258513450623
training loss: 0.1271003782749176
training loss: 0.12700125575065613
training loss: 0.6291091442108154
training loss: 0.12705203890800476
training loss: 0.12784455716609955
training loss: 0.1270771324634552
training loss: 0.1270236372947693
training loss: 0.1344885528087616
training loss: 0.1276436150074005
training loss: 0.6271562576293945
training loss: 0.12704908847808838
training loss: 0.12751100957393646
training loss: 0.1269707977771759
training loss: 0.13018712401390076
training loss: 0.3773777484893799
training loss: 0.12699222564697266
training loss: 0.1291390061378479
training loss: 0.12705031037330627
training loss: 0.12716256082057953
training loss: 0.12703147530555725
training loss: 0.12708662450313568
training loss: 0.1272195726633072
training loss: 0.12710410356521606
training loss: 0.13184379041194916
training loss: 0.12693333625793457
training loss: 0.126991406083107
training loss: 0.12695644795894623
training loss: 0.1273803412914276
training loss: 0.12919776141643524
training loss: 0.1269797533750534
training loss: 0.12709538638591766
training loss: 0.1270882934331894
training loss: 0.37699633836746216
training loss: 0.12713812291622162
training loss: 0.127178356051445
training loss: 0.12715066969394684
training loss: 0.1272142231464386
training loss: 0.12762343883514404
training loss: 0.37697356939315796
training loss: 0.1367756575345993
training loss: 0.12720370292663574
training loss: 0.3769124746322632
training loss: 0.1287476122379303
training loss: 0.37677323818206787
training loss: 0.1273835003376007
training loss: 0.12973672151565552
training loss: 0.12703444063663483
training loss: 0.1273059844970703
training loss: 0.3770861327648163
training loss: 0.12821418046951294
training loss: 0.12710048258304596
training loss: 0.1272088587284088
training loss: 0.3771449625492096
training loss: 0.37748482823371887
training loss: 0.12710005044937134
training loss: 0.12712061405181885
training loss: 0.1272083818912506
training loss: 0.12709957361221313
training loss: 0.1269441694021225
training loss: 0.3783385455608368
training loss: 0.12722723186016083
training loss: 0.3771297037601471
training loss: 0.12875410914421082
training loss: 0.1270839273929596
training loss: 0.12702365219593048
training loss: 0.1269938200712204
training loss: 0.1282980591058731
training loss: 0.3769887387752533
training loss: 0.1290731430053711
training loss: 0.12715591490268707
training loss: 0.1272820234298706
training loss: 0.12745864689350128
training loss: 0.12695690989494324
training loss: 0.15347504615783691
training loss: 0.37698739767074585
training loss: 0.1272486299276352
training loss: 0.12770392000675201
training loss: 0.127134308218956
training loss: 0.12781554460525513
training loss: 0.6277341842651367
training loss: 0.12696519494056702
training loss: 0.127727672457695
training loss: 0.12725014984607697
training loss: 0.3759952187538147
training loss: 0.13216178119182587
training loss: 0.12701822817325592
training loss: 0.12715715169906616
training loss: 0.12696194648742676
training loss: 0.37823551893234253
training loss: 0.1272958368062973
training loss: 0.1279943883419037
training loss: 0.1275196373462677
training loss: 0.12755230069160461
training loss: 0.3769209682941437
training loss: 0.1270584762096405
training loss: 0.37713855504989624
training loss: 0.12694944441318512
training loss: 0.12718477845191956
training loss: 0.1271674633026123
training loss: 0.12703962624073029
training loss: 0.3783402144908905
training loss: 0.126948744058609
training loss: 0.1281999796628952
training loss: 0.12707839906215668
training loss: 0.12716123461723328
training loss: 0.12695446610450745
training loss: 0.12713852524757385
training loss: 0.37444788217544556
training loss: 0.3769088685512543
training loss: 0.12695598602294922
training loss: 0.12721982598304749
training loss: 0.3771555423736572
training loss: 0.1282084733247757
training loss: 0.12874773144721985
training loss: 0.6270599365234375
training loss: 0.12695243954658508
training loss: 0.37686893343925476
training loss: 0.12719960510730743
training loss: 0.12723182141780853
training loss: 0.12726344168186188
training loss: 0.37695643305778503
training loss: 0.12716005742549896
training loss: 0.12738542258739471
training loss: 0.1273835301399231
training loss: 0.12752005457878113
training loss: 0.1270252913236618
training loss: 0.12774132192134857
training loss: 0.1270604133605957
training loss: 0.12949201464653015
training loss: 0.12735778093338013
training loss: 0.1271330714225769
training loss: 0.1274082064628601
training loss: 0.12723444402217865
training loss: 0.37704411149024963
training loss: 0.13056594133377075
training loss: 0.1274413764476776
training loss: 0.3770523965358734
training loss: 0.12695981562137604
training loss: 0.37719252705574036
training loss: 0.12710797786712646
training loss: 0.3770231008529663
training loss: 0.1269582062959671
training loss: 0.12932106852531433
training loss: 0.12709710001945496
training loss: 0.13178591430187225
training loss: 0.12787969410419464
training loss: 0.1279282569885254
training loss: 0.12696513533592224
training loss: 0.37724781036376953
training loss: 0.6272478103637695
training loss: 0.12703776359558105
training loss: 0.12703068554401398
training loss: 0.12697665393352509
training loss: 0.3778800964355469
training loss: 0.37567004561424255
training loss: 0.3775550425052643
training loss: 0.1271711140871048
training loss: 0.12693771719932556
training loss: 0.12700681388378143
training loss: 0.12706471979618073
training loss: 0.12743797898292542
training loss: 0.12708701193332672
training loss: 0.12705671787261963
training loss: 0.12697821855545044
training loss: 0.3770544230937958
training loss: 0.1272841989994049
training loss: 0.12702256441116333
training loss: 0.12740124762058258
training loss: 0.1274413764476776
training loss: 0.12781888246536255
training loss: 0.12718842923641205
training loss: 0.12701717019081116
training loss: 0.3790801167488098
training loss: 0.12718626856803894
training loss: 0.12704181671142578
training loss: 0.3767578601837158
training loss: 0.12697649002075195
training loss: 0.1273321807384491
training loss: 0.12767848372459412
training loss: 0.1459813266992569
training loss: 0.3762165904045105
training loss: 0.12698428332805634
training loss: 0.12699592113494873
training loss: 0.12708325684070587
training loss: 0.12847517430782318
training loss: 0.12695540487766266
training loss: 0.12714451551437378
training loss: 0.12744982540607452
training loss: 0.12699176371097565
training loss: 0.12698176503181458
training loss: 0.3764858543872833
training loss: 0.3769666254520416
training loss: 0.13891874253749847
training loss: 0.12712644040584564
training loss: 0.12693972885608673
training loss: 0.12772049009799957
training loss: 0.12737488746643066
training loss: 0.12702663242816925
training loss: 0.1389673352241516
training loss: 0.3770630657672882
training loss: 0.3771078884601593
training loss: 0.12697967886924744
training loss: 0.3768976330757141
training loss: 0.6269358396530151
training loss: 0.3769455552101135
training loss: 0.3772840201854706
training loss: 0.12696877121925354
training loss: 0.37706267833709717
training loss: 0.1273641735315323
training loss: 0.12699300050735474
training loss: 0.12797167897224426
training loss: 0.3769591450691223
training loss: 0.12726420164108276
training loss: 0.6207727789878845
training loss: 0.12841838598251343
training loss: 0.37667232751846313
training loss: 0.3770539462566376
training loss: 0.127505823969841
training loss: 0.37747877836227417
training loss: 0.12696415185928345
training loss: 0.12696151435375214
training loss: 0.3770201802253723
training loss: 0.37746888399124146
training loss: 0.12712088227272034
training loss: 0.12722249329090118
training loss: 0.1269412636756897
training loss: 0.3764866888523102
training loss: 0.12741056084632874
training loss: 0.12745502591133118
training loss: 0.12705418467521667
training loss: 0.12933966517448425
training loss: 0.12694790959358215
training loss: 0.12730176746845245
training loss: 0.12863266468048096
training loss: 0.12701627612113953
training loss: 0.1275150328874588
training loss: 0.37740686535835266
training loss: 0.12697747349739075
training loss: 0.3770495057106018
training loss: 0.12699542939662933
training loss: 0.3769947290420532
training loss: 0.12729930877685547
training loss: 0.12700815498828888
training loss: 0.12695461511611938
training loss: 0.3773829936981201
training loss: 0.37702131271362305
training loss: 0.12699496746063232
training loss: 0.1269800364971161
training loss: 0.12696120142936707
training loss: 0.12711533904075623
training loss: 0.1271783709526062
training loss: 0.1269480139017105
training loss: 0.12792673707008362
training loss: 0.3769734799861908
training loss: 0.37653398513793945
training loss: 0.37702634930610657
training loss: 0.3772580921649933
training loss: 0.12779226899147034
training loss: 0.3787408769130707
training loss: 0.12712058424949646
training loss: 0.37704503536224365
training loss: 0.1270483136177063
training loss: 0.37698888778686523
training loss: 0.12702319025993347
training loss: 0.12717391550540924
training loss: 0.37693387269973755
training loss: 0.12698140740394592
training loss: 0.37672513723373413
training loss: 0.12697555124759674
training loss: 0.12714079022407532
training loss: 0.12726691365242004
training loss: 0.12705934047698975
training loss: 0.1272052526473999
training loss: 0.1269797384738922
training loss: 0.12884357571601868
training loss: 0.12721192836761475
training loss: 0.12693646550178528
training loss: 0.1269555687904358
training loss: 0.3768847584724426
training loss: 0.12696847319602966
training loss: 0.12702298164367676
training loss: 0.12717604637145996
training loss: 0.12751926481723785
training loss: 0.1270168274641037
training loss: 0.12729012966156006
training loss: 0.37756359577178955
training loss: 0.12708643078804016
training loss: 0.3769872188568115
training loss: 0.12709522247314453
training loss: 0.1269611269235611
training loss: 0.12696149945259094
training loss: 0.3769940137863159
training loss: 0.12727639079093933
training loss: 0.12701475620269775
training loss: 0.1272173970937729
training loss: 0.12695473432540894
training loss: 0.3769308924674988
training loss: 0.12728704512119293
training loss: 0.12865597009658813
training loss: 0.12699560821056366
training loss: 0.12696035206317902
training loss: 0.12708015739917755
training loss: 0.12715959548950195
training loss: 0.12706875801086426
training loss: 0.12695160508155823
training loss: 0.12721119821071625
training loss: 0.12700983881950378
training loss: 0.12701532244682312
training loss: 0.12697720527648926
training loss: 0.12694151699543
training loss: 0.37693578004837036
training loss: 0.1269666850566864
training loss: 0.3771709203720093
training loss: 0.126967191696167
training loss: 0.12697240710258484
training loss: 0.37720248103141785
training loss: 0.37700891494750977
training loss: 0.12700848281383514
training loss: 0.12764005362987518
training loss: 0.12706145644187927
training loss: 0.37695440649986267
training loss: 0.3771056532859802
training loss: 0.12699463963508606
training loss: 0.12837627530097961
training loss: 0.127390518784523
training loss: 0.1269596815109253
training loss: 0.3769838213920593
training loss: 0.37706708908081055
training loss: 0.12890633940696716
training loss: 0.1271362006664276
training loss: 0.1269439160823822
training loss: 0.12713435292243958
training loss: 0.12698493897914886
training loss: 0.1272703856229782
training loss: 0.1275315284729004
training loss: 0.12710145115852356
training loss: 0.1269473433494568
training loss: 0.3769412636756897
training loss: 0.12767834961414337
training loss: 0.1269686371088028
training loss: 0.1270885318517685
training loss: 0.12702129781246185
training loss: 0.12696705758571625
training loss: 0.12707822024822235
training loss: 0.12693867087364197
training loss: 0.378009557723999
training loss: 0.12714992463588715
training loss: 0.12693387269973755
training loss: 0.3769288957118988
training loss: 0.6263691782951355
training loss: 0.12713295221328735
training loss: 0.12700685858726501
training loss: 2.1268386840820312
Average loss at epoch 2: 0.2003274224289171
training loss: 0.12693257629871368
training loss: 0.3770899176597595
training loss: 0.3769739270210266
training loss: 0.12704038619995117
training loss: 0.12701621651649475
training loss: 0.12694448232650757
training loss: 0.3769380450248718
training loss: 0.12703736126422882
training loss: 0.1269422322511673
training loss: 0.12701168656349182
training loss: 0.12868347764015198
training loss: 0.12713900208473206
training loss: 0.12722326815128326
training loss: 0.12700842320919037
training loss: 0.12720715999603271
training loss: 0.12695494294166565
training loss: 0.12703311443328857
training loss: 0.37703055143356323
training loss: 0.12792626023292542
training loss: 0.37700462341308594
training loss: 0.12706005573272705
training loss: 0.12741215527057648
training loss: 0.1270308941602707
training loss: 0.37695780396461487
training loss: 0.37707629799842834
training loss: 0.12718306481838226
training loss: 0.12709344923496246
training loss: 0.12700900435447693
training loss: 0.12707741558551788
training loss: 0.12703028321266174
training loss: 0.37713924050331116
training loss: 0.3769643008708954
training loss: 0.1269829124212265
training loss: 0.1272982954978943
training loss: 0.1277790069580078
training loss: 0.12706322968006134
training loss: 0.12704436480998993
training loss: 0.12704718112945557
training loss: 0.12694303691387177
training loss: 0.12701743841171265
training loss: 0.12706847488880157
training loss: 0.37693411111831665
training loss: 0.1272522360086441
training loss: 0.126987025141716
training loss: 0.3770707845687866
training loss: 0.12697817385196686
training loss: 0.37718337774276733
training loss: 0.12698830664157867
training loss: 0.1271333396434784
training loss: 0.12695367634296417
training loss: 0.3770909905433655
training loss: 0.12694969773292542
training loss: 0.12702929973602295
training loss: 0.12704671919345856
training loss: 0.12696830928325653
training loss: 0.12710243463516235
training loss: 0.1271074414253235
training loss: 0.6265845894813538
training loss: 0.12750773131847382
training loss: 0.12712760269641876
training loss: 0.12741884589195251
training loss: 0.1270933300256729
training loss: 0.12702181935310364
training loss: 0.3769841492176056
training loss: 0.12727735936641693
training loss: 0.12695053219795227
training loss: 0.12739458680152893
training loss: 0.12695527076721191
training loss: 0.1269431710243225
training loss: 0.12697722017765045
training loss: 0.12718692421913147
training loss: 0.12706801295280457
training loss: 0.12693731486797333
training loss: 0.3769685626029968
training loss: 0.12791815400123596
training loss: 0.3770695626735687
training loss: 0.1278444230556488
training loss: 0.12698698043823242
training loss: 0.37790992856025696
training loss: 0.37697088718414307
training loss: 0.12982751429080963
training loss: 0.13232219219207764
training loss: 0.1274121105670929
training loss: 0.37697041034698486
training loss: 0.12693385779857635
training loss: 0.12696367502212524
training loss: 0.12717266380786896
training loss: 0.627009391784668
training loss: 0.12705391645431519
training loss: 0.6270686388015747
training loss: 0.12746869027614594
training loss: 0.1270308494567871
training loss: 0.12838470935821533
training loss: 0.13227751851081848
training loss: 1.1265162229537964
training loss: 0.12700627744197845
training loss: 0.1269325166940689
training loss: 0.1270933449268341
training loss: 0.3769965171813965
training loss: 0.3770783841609955
training loss: 0.12738409638404846
training loss: 0.12701746821403503
training loss: 0.12702953815460205
training loss: 0.12708446383476257
training loss: 0.12694121897220612
training loss: 0.37721988558769226
training loss: 0.12695437669754028
training loss: 0.1270606815814972
training loss: 0.3768859803676605
training loss: 0.1269511729478836
training loss: 0.3769954741001129
training loss: 0.3771806061267853
training loss: 0.12707851827144623
training loss: 0.12716177105903625
training loss: 0.37683892250061035
training loss: 0.12777365744113922
training loss: 0.12693846225738525
training loss: 0.12695826590061188
training loss: 0.12743952870368958
training loss: 0.1269506812095642
training loss: 0.1270238161087036
training loss: 0.12704400718212128
training loss: 0.1271490901708603
training loss: 0.12703503668308258
training loss: 0.12697049975395203
training loss: 0.12715110182762146
training loss: 0.12705214321613312
training loss: 0.12697641551494598
training loss: 0.12699168920516968
training loss: 0.12693679332733154
training loss: 0.12696027755737305
training loss: 0.1270984709262848
training loss: 0.12780921161174774
training loss: 0.12752623856067657
training loss: 0.12695135176181793
training loss: 0.37752142548561096
training loss: 0.12750059366226196
training loss: 0.3771301507949829
training loss: 0.3768600821495056
training loss: 0.12707358598709106
training loss: 0.12705890834331512
training loss: 0.626940131187439
training loss: 0.12740910053253174
training loss: 0.12703455984592438
training loss: 0.6260955929756165
training loss: 0.37686145305633545
training loss: 0.12697279453277588
training loss: 0.1269521415233612
training loss: 0.37691396474838257
training loss: 0.3691096305847168
training loss: 0.1269819289445877
training loss: 0.37693673372268677
training loss: 0.12742774188518524
training loss: 0.37692874670028687
training loss: 0.12698635458946228
training loss: 0.12702424824237823
training loss: 0.12697076797485352
training loss: 0.37612301111221313
training loss: 0.12722453474998474
training loss: 0.12700895965099335
training loss: 0.12697570025920868
training loss: 0.376076340675354
training loss: 0.37708190083503723
training loss: 0.12724685668945312
training loss: 0.1269514560699463
training loss: 0.12708012759685516
training loss: 0.12694819271564484
training loss: 0.12709397077560425
training loss: 0.1271025836467743
training loss: 0.12716859579086304
training loss: 0.1275116205215454
training loss: 0.12725278735160828
training loss: 0.12698301672935486
training loss: 0.3766615092754364
training loss: 0.3769410252571106
training loss: 0.12697181105613708
training loss: 0.12752212584018707
training loss: 0.12722820043563843
training loss: 0.626914381980896
training loss: 0.12722079455852509
training loss: 0.376883327960968
training loss: 0.6269758939743042
training loss: 0.1272023469209671
training loss: 0.3763525187969208
training loss: 0.12695495784282684
training loss: 0.12744079530239105
training loss: 0.12710025906562805
training loss: 0.3769363462924957
training loss: 0.37705227732658386
training loss: 0.14182955026626587
training loss: 0.1270429790019989
training loss: 0.1273089498281479
training loss: 0.1271170675754547
training loss: 0.127696231007576
training loss: 0.127249076962471
training loss: 0.12701310217380524
training loss: 0.12714269757270813
training loss: 0.12695953249931335
training loss: 0.3769463002681732
training loss: 0.12697263062000275
training loss: 0.1270609349012375
training loss: 0.12712633609771729
training loss: 0.1269655078649521
training loss: 0.12704265117645264
training loss: 0.1269504874944687
training loss: 0.376980721950531
training loss: 0.12695807218551636
training loss: 0.12715083360671997
training loss: 0.12696044147014618
training loss: 0.37722277641296387
training loss: 0.1270274519920349
training loss: 0.1269499510526657
training loss: 0.1272345781326294
training loss: 0.12722116708755493
training loss: 0.12699399888515472
training loss: 0.1272912323474884
training loss: 0.3788799047470093
training loss: 0.3770016133785248
training loss: 0.1269349753856659
training loss: 0.1271723508834839
training loss: 0.12694492936134338
training loss: 0.3770260512828827
training loss: 0.3769347071647644
training loss: 0.1276019960641861
training loss: 0.1273261159658432
training loss: 0.1270076334476471
training loss: 0.12709307670593262
training loss: 0.12717843055725098
training loss: 0.12714049220085144
training loss: 0.1269540935754776
training loss: 0.3767872750759125
training loss: 0.126968652009964
training loss: 0.12694628536701202
training loss: 0.12695229053497314
training loss: 0.1269371509552002
training loss: 0.12693847715854645
training loss: 0.127019003033638
training loss: 0.1271342635154724
training loss: 0.12694765627384186
training loss: 0.12695826590061188
training loss: 0.12701500952243805
training loss: 0.1269933581352234
training loss: 0.12693962454795837
training loss: 0.12718458473682404
training loss: 0.37729161977767944
training loss: 0.12697070837020874
training loss: 0.37698695063591003
training loss: 0.12694740295410156
training loss: 0.12829945981502533
training loss: 0.1269386112689972
training loss: 0.1274028867483139
training loss: 0.3769467771053314
training loss: 0.1269485354423523
training loss: 0.12733657658100128
training loss: 0.3771471679210663
training loss: 0.1270284354686737
training loss: 0.12748965620994568
training loss: 0.12694865465164185
training loss: 0.12831729650497437
training loss: 0.12732267379760742
training loss: 0.12697511911392212
training loss: 0.12694339454174042
training loss: 0.12696537375450134
training loss: 0.1279095858335495
training loss: 0.12710371613502502
training loss: 0.1278790533542633
training loss: 0.1270342767238617
training loss: 0.12694530189037323
training loss: 0.12716111540794373
training loss: 0.12696608901023865
training loss: 0.12760603427886963
training loss: 0.12697122991085052
training loss: 0.3768746554851532
training loss: 0.12699535489082336
training loss: 0.12699300050735474
training loss: 0.126982182264328
training loss: 0.12695267796516418
training loss: 0.3769543468952179
training loss: 0.3767380714416504
training loss: 0.12696518003940582
training loss: 0.12701645493507385
training loss: 0.1269378960132599
training loss: 0.1269686222076416
training loss: 0.12724252045154572
training loss: 0.1270357221364975
training loss: 0.1270420253276825
training loss: 0.12697051465511322
training loss: 0.1271023452281952
training loss: 0.12706662714481354
training loss: 0.1269465535879135
training loss: 0.37708744406700134
training loss: 0.12698455154895782
training loss: 0.12696145474910736
training loss: 0.3781428933143616
training loss: 0.3765835165977478
training loss: 0.1276574581861496
training loss: 0.12697558104991913
training loss: 0.12709297239780426
training loss: 0.3782176673412323
training loss: 0.12696954607963562
training loss: 0.3769198954105377
training loss: 0.1271863877773285
training loss: 0.12693867087364197
training loss: 0.377086341381073
training loss: 0.12706881761550903
training loss: 0.12704062461853027
training loss: 0.1269906908273697
training loss: 0.12716692686080933
training loss: 0.12715399265289307
training loss: 0.12722866237163544
training loss: 0.12697097659111023
training loss: 0.126966655254364
training loss: 0.12716074287891388
training loss: 0.12786072492599487
training loss: 0.12693330645561218
training loss: 0.12694217264652252
training loss: 0.12755906581878662
training loss: 0.126989483833313
training loss: 0.12694725394248962
training loss: 0.37695130705833435
training loss: 0.1273302584886551
training loss: 0.12694168090820312
training loss: 0.12732674181461334
training loss: 0.12704814970493317
training loss: 0.37694495916366577
training loss: 0.1269415318965912
training loss: 0.1269608438014984
training loss: 0.3769913613796234
training loss: 0.37697604298591614
training loss: 0.3770436644554138
training loss: 0.1269780546426773
training loss: 0.12764379382133484
training loss: 0.12726444005966187
training loss: 0.1279439777135849
training loss: 0.3773387670516968
training loss: 0.3760540783405304
training loss: 0.12695446610450745
training loss: 0.12700563669204712
training loss: 0.6269495487213135
training loss: 0.12694300711154938
training loss: 0.3769301772117615
training loss: 0.6269717216491699
training loss: 0.1270180195569992
training loss: 0.37718382477760315
training loss: 0.3769582211971283
training loss: 0.12698876857757568
training loss: 0.12698902189731598
training loss: 0.12694069743156433
training loss: 0.37698543071746826
training loss: 0.12701182067394257
training loss: 0.12695720791816711
training loss: 0.12707233428955078
training loss: 0.12815043330192566
training loss: 0.12696573138237
training loss: 0.12705354392528534
training loss: 0.12694813311100006
training loss: 0.12733300030231476
training loss: 0.12698417901992798
training loss: 0.3769528567790985
training loss: 0.1270371377468109
training loss: 0.37696078419685364
training loss: 0.12725108861923218
training loss: 0.12701410055160522
training loss: 0.12693004310131073
Average loss at epoch 3: 0.19515113921447114
training loss: 0.12693488597869873
training loss: 0.1271137148141861
training loss: 0.12698885798454285
training loss: 0.37711793184280396
training loss: 0.12748375535011292
training loss: 0.1270514875650406
training loss: 0.12697379291057587
training loss: 0.12702618539333344
training loss: 0.12694844603538513
training loss: 0.1272352933883667
training loss: 0.1269492655992508
training loss: 0.12695923447608948
training loss: 0.12697833776474
training loss: 0.12694016098976135
training loss: 0.12718985974788666
training loss: 0.12899698317050934
training loss: 0.12703116238117218
training loss: 0.1270870864391327
training loss: 0.12712334096431732
training loss: 0.1271495223045349
training loss: 0.12693744897842407
training loss: 0.12701651453971863
training loss: 0.13494788110256195
training loss: 0.12698276340961456
training loss: 0.1270541101694107
training loss: 0.12778913974761963
training loss: 0.1269296407699585
training loss: 0.12694039940834045
training loss: 0.1269434690475464
training loss: 0.1270015388727188
training loss: 0.12741076946258545
training loss: 0.126989483833313
training loss: 0.12711097300052643
training loss: 0.12693485617637634
training loss: 0.1269422173500061
training loss: 0.1269523948431015
training loss: 0.37715649604797363
training loss: 0.3769596815109253
training loss: 0.37710022926330566
training loss: 0.12695655226707458
training loss: 0.12709149718284607
training loss: 0.12699851393699646
training loss: 0.12708288431167603
training loss: 0.12712879478931427
training loss: 0.12696047127246857
training loss: 0.37700989842414856
training loss: 0.1271790862083435
training loss: 0.12693315744400024
training loss: 0.12695500254631042
training loss: 0.1276942491531372
training loss: 0.3771957755088806
training loss: 0.1270129233598709
training loss: 0.3769266605377197
training loss: 0.12709981203079224
training loss: 0.12699218094348907
training loss: 0.12692853808403015
training loss: 0.12702161073684692
training loss: 0.37742748856544495
training loss: 0.12704364955425262
training loss: 0.12736116349697113
training loss: 0.12710024416446686
training loss: 0.3769972324371338
training loss: 0.12697337567806244
training loss: 0.12702004611492157
training loss: 0.12710338830947876
training loss: 0.3767905831336975
training loss: 0.12694093585014343
training loss: 0.12717920541763306
training loss: 0.12704437971115112
training loss: 0.12694920599460602
training loss: 0.3772413432598114
training loss: 0.3770861327648163
training loss: 0.12704727053642273
training loss: 0.1270221769809723
training loss: 0.12730833888053894
training loss: 0.3769882619380951
training loss: 0.37696072459220886
training loss: 0.1271146982908249
training loss: 0.12710458040237427
training loss: 0.12693879008293152
training loss: 0.12694305181503296
training loss: 0.12696731090545654
training loss: 0.12694157660007477
training loss: 0.12696486711502075
training loss: 0.1269700825214386
training loss: 0.12705515325069427
training loss: 0.12699458003044128
training loss: 0.1270715296268463
training loss: 0.3769138753414154
training loss: 0.12701299786567688
training loss: 0.12701641023159027
training loss: 0.12699177861213684
training loss: 0.12710048258304596
training loss: 0.1269441843032837
training loss: 0.3768630027770996
training loss: 0.12700212001800537
training loss: 0.1269730180501938
training loss: 0.1269443929195404
training loss: 0.3769457936286926
training loss: 0.12747526168823242
training loss: 0.12693361937999725
training loss: 0.12696340680122375
training loss: 0.12694092094898224
training loss: 0.3769574463367462
training loss: 0.12699458003044128
training loss: 0.3767523169517517
training loss: 0.12694567441940308
training loss: 0.37693241238594055
training loss: 0.1269523650407791
training loss: 0.3769605755805969
training loss: 0.12697191536426544
training loss: 0.37693551182746887
training loss: 0.12693682312965393
training loss: 0.12730549275875092
training loss: 0.12696880102157593
training loss: 0.12699425220489502
training loss: 0.1269865781068802
training loss: 0.37693607807159424
training loss: 0.12693798542022705
training loss: 0.37601375579833984
training loss: 0.12715201079845428
training loss: 0.12700748443603516
training loss: 0.1269356906414032
training loss: 0.12697067856788635
training loss: 0.12696166336536407
training loss: 0.1269577294588089
training loss: 0.12732139229774475
training loss: 0.1274319291114807
training loss: 0.37712961435317993
training loss: 0.12702922523021698
training loss: 0.12843023240566254
training loss: 0.12857399880886078
training loss: 0.3770168423652649
training loss: 0.37791162729263306
training loss: 0.12707291543483734
training loss: 0.37705177068710327
training loss: 0.1269538700580597
training loss: 0.12719640135765076
training loss: 0.12694866955280304
training loss: 0.1270773857831955
training loss: 0.12730665504932404
training loss: 0.12711617350578308
training loss: 0.12697702646255493
training loss: 0.12693871557712555
training loss: 0.12694741785526276
training loss: 0.1269669383764267
training loss: 0.3773326575756073
training loss: 0.1269625574350357
training loss: 0.12698401510715485
training loss: 0.12695129215717316
training loss: 0.1269817054271698
training loss: 0.3769378364086151
training loss: 0.12695427238941193
training loss: 0.13821792602539062
training loss: 0.3769722580909729
training loss: 0.12694573402404785
training loss: 0.376968652009964
training loss: 0.12709341943264008
training loss: 0.12709884345531464
training loss: 0.12735027074813843
training loss: 0.12721751630306244
training loss: 0.12697584927082062
training loss: 0.12694092094898224
training loss: 0.12710902094841003
training loss: 0.12694582343101501
training loss: 0.12695135176181793
training loss: 0.37694650888442993
training loss: 0.12695278227329254
training loss: 0.37700313329696655
training loss: 0.12750676274299622
training loss: 0.37715625762939453
training loss: 0.12694618105888367
training loss: 0.12698650360107422
training loss: 0.12701350450515747
training loss: 0.37695589661598206
training loss: 0.37695297598838806
training loss: 0.37728995084762573
training loss: 0.1269458830356598
training loss: 0.12700535356998444
training loss: 0.1269315928220749
training loss: 0.1269417256116867
training loss: 0.3758290708065033
training loss: 0.37693849205970764
training loss: 0.3769947290420532
training loss: 0.12695083022117615
training loss: 0.12706777453422546
training loss: 0.12744902074337006
training loss: 0.6269409656524658
training loss: 0.37698855996131897
training loss: 0.12712839245796204
training loss: 0.37694284319877625
training loss: 0.12700745463371277
training loss: 0.37690234184265137
training loss: 0.1269543319940567
training loss: 0.3769285976886749
training loss: 0.1270129233598709
training loss: 0.12713265419006348
training loss: 0.12693406641483307
training loss: 0.376936674118042
training loss: 0.1270131766796112
training loss: 0.1269368678331375
training loss: 0.12694357335567474
training loss: 0.12698251008987427
training loss: 0.12698225677013397
training loss: 0.3769341707229614
training loss: 0.12695281207561493
training loss: 0.3769311010837555
training loss: 0.12729698419570923
training loss: 0.12693364918231964
training loss: 0.12694665789604187
training loss: 0.12695851922035217
training loss: 0.12700273096561432
training loss: 0.1271059513092041
training loss: 0.3769820034503937
training loss: 0.626489520072937
training loss: 0.12708038091659546
training loss: 0.12693673372268677
training loss: 0.37644901871681213
training loss: 0.37700897455215454
training loss: 0.12697367370128632
training loss: 0.3769287168979645
training loss: 0.3769379258155823
training loss: 0.37664780020713806
training loss: 0.37692520022392273
training loss: 0.1269732117652893
training loss: 0.1271226406097412
training loss: 0.12726373970508575
training loss: 0.37705275416374207
training loss: 0.37694427371025085
training loss: 0.37710925936698914
training loss: 0.12706401944160461
training loss: 0.37693098187446594
training loss: 0.12707003951072693
training loss: 0.12698861956596375
training loss: 0.12693865597248077
training loss: 0.3770255446434021
training loss: 0.3769337832927704
training loss: 0.12694701552391052
training loss: 0.12693317234516144
training loss: 0.12697595357894897
training loss: 0.12694598734378815
training loss: 0.15378424525260925
training loss: 0.1269567310810089
training loss: 0.12693317234516144
training loss: 0.12699580192565918
training loss: 0.12698251008987427
training loss: 0.12697574496269226
training loss: 0.3775079846382141
training loss: 0.12701524794101715
training loss: 0.3781396448612213
training loss: 0.12855039536952972
training loss: 0.3769097924232483
training loss: 0.12693487107753754
training loss: 0.1269429624080658
training loss: 0.12693557143211365
training loss: 0.12735310196876526
training loss: 0.1269381195306778
training loss: 0.12708322703838348
training loss: 0.12705601751804352
training loss: 0.12700054049491882
training loss: 0.3769286870956421
training loss: 0.12777508795261383
training loss: 0.12694597244262695
training loss: 0.12701921164989471
training loss: 0.12693952023983002
training loss: 0.1269407421350479
training loss: 0.376977801322937
training loss: 0.37692999839782715
training loss: 0.12745103240013123
training loss: 0.12695622444152832
training loss: 0.12698622047901154
training loss: 0.6277503967285156
training loss: 0.12693360447883606
training loss: 0.12693250179290771
training loss: 0.12716133892536163
training loss: 0.1269555389881134
training loss: 0.3769470751285553
training loss: 0.12702703475952148
training loss: 0.6274631023406982
training loss: 0.3769734799861908
training loss: 0.12695884704589844
training loss: 0.12697528302669525
training loss: 0.3769780397415161
training loss: 0.3769267797470093
training loss: 0.1271897405385971
training loss: 0.127324640750885
training loss: 0.1270325779914856
training loss: 0.1281505823135376
training loss: 0.1275179386138916
training loss: 0.377322256565094
training loss: 0.12703171372413635
training loss: 0.12710829079151154
training loss: 0.37711194157600403
training loss: 0.126947283744812
training loss: 0.12693823873996735
training loss: 0.12708477675914764
training loss: 0.127001091837883
training loss: 0.12693199515342712
training loss: 0.12747767567634583
training loss: 0.37649571895599365
training loss: 0.6268273591995239
training loss: 0.12693804502487183
training loss: 0.6269620060920715
training loss: 0.1275094896554947
training loss: 0.1269679218530655
training loss: 0.6270328164100647
training loss: 0.12697508931159973
training loss: 0.12694138288497925
training loss: 0.1272154003381729
training loss: 0.12713810801506042
training loss: 0.37768951058387756
training loss: 0.37696531414985657
training loss: 0.12735310196876526
training loss: 0.1271381676197052
training loss: 0.12705335021018982
training loss: 0.12700390815734863
training loss: 0.12693054974079132
training loss: 0.12706495821475983
training loss: 0.12693585455417633
training loss: 0.12694719433784485
training loss: 0.12696050107479095
training loss: 0.12695857882499695
training loss: 0.37706977128982544
training loss: 0.12694160640239716
training loss: 0.12703076004981995
training loss: 0.12740308046340942
training loss: 0.12693443894386292
training loss: 0.3769306540489197
training loss: 0.12693388760089874
training loss: 0.1272355318069458
training loss: 0.12696923315525055
training loss: 0.3774455785751343
training loss: 0.12693704664707184
training loss: 0.12693022191524506
training loss: 0.12697690725326538
training loss: 0.12694351375102997
training loss: 0.12694859504699707
training loss: 0.12695132195949554
training loss: 0.3769484758377075
training loss: 0.1269383281469345
training loss: 0.37693503499031067
training loss: 0.12704837322235107
training loss: 0.37694472074508667
training loss: 0.12693250179290771
training loss: 0.1269679069519043
training loss: 0.3769536316394806
training loss: 0.1269298940896988
training loss: 0.12732313573360443
training loss: 0.37704968452453613
training loss: 0.12886843085289001
training loss: 0.12703391909599304
training loss: 0.37708723545074463
training loss: 0.12696689367294312
training loss: 0.12709611654281616
training loss: 0.12696245312690735
training loss: 0.12693068385124207
training loss: 0.12701764702796936
training loss: 0.12699830532073975
training loss: 0.12693729996681213
training loss: 0.12769189476966858
training loss: 0.1270640343427658
training loss: 0.12695078551769257
training loss: 0.1269749104976654
training loss: 0.12692856788635254
Average loss at epoch 4: 0.19518165024263517
training loss: 0.1269529163837433
training loss: 0.37692904472351074
training loss: 0.12702903151512146
training loss: 0.12694981694221497
training loss: 0.1269356608390808
training loss: 0.37687939405441284
training loss: 0.12696750462055206
training loss: 0.1270073652267456
training loss: 0.12694987654685974
training loss: 0.12706467509269714
training loss: 0.12694314122200012
training loss: 0.12715964019298553
training loss: 0.1269480288028717
training loss: 0.37694141268730164
training loss: 0.1270940899848938
training loss: 0.37797272205352783
training loss: 0.37691035866737366
training loss: 0.12708449363708496
training loss: 0.12700535356998444
training loss: 0.1270258128643036
training loss: 0.37589919567108154
training loss: 0.3771549165248871
training loss: 0.12701769173145294
training loss: 0.12694460153579712
training loss: 0.12700241804122925
training loss: 0.376954048871994
training loss: 0.12694057822227478
training loss: 0.12693297863006592
training loss: 0.1269298642873764
training loss: 0.12701749801635742
training loss: 0.12702576816082
training loss: 0.1270235925912857
training loss: 0.12711495161056519
training loss: 0.1269463449716568
training loss: 0.12695033848285675
training loss: 0.3769919276237488
training loss: 0.12692995369434357
training loss: 0.1269710659980774
training loss: 0.3769347071647644
training loss: 0.3772521913051605
training loss: 0.12694191932678223
training loss: 0.12693879008293152
training loss: 0.37698960304260254
training loss: 0.6270323991775513
training loss: 0.12694382667541504
training loss: 0.37692204117774963
training loss: 0.12706980109214783
training loss: 0.12693940103054047
training loss: 0.12693271040916443
training loss: 0.12723278999328613
training loss: 0.37693580985069275
training loss: 0.1269824206829071
training loss: 0.3769501745700836
training loss: 0.12699514627456665
training loss: 0.37590765953063965
training loss: 0.12721778452396393
training loss: 0.12704867124557495
training loss: 0.12694568932056427
training loss: 0.13033385574817657
training loss: 0.12693776190280914
training loss: 0.1269838809967041
training loss: 0.12708780169487
training loss: 0.37661421298980713
training loss: 0.12703227996826172
training loss: 0.12695035338401794
training loss: 0.12693969905376434
training loss: 0.3769513964653015
training loss: 0.1269499808549881
training loss: 0.12801893055438995
training loss: 0.12697136402130127
training loss: 0.12694644927978516
training loss: 0.12693127989768982
training loss: 0.12693527340888977
training loss: 0.12696070969104767
training loss: 0.6267704963684082
training loss: 0.12726640701293945
training loss: 0.12696677446365356
training loss: 0.12693659961223602
training loss: 0.6269756555557251
training loss: 0.12733645737171173
training loss: 0.1269398033618927
training loss: 0.37685930728912354
training loss: 0.37697869539260864
training loss: 0.12818536162376404
training loss: 0.1269468367099762
training loss: 0.12693646550178528
training loss: 0.12701064348220825
training loss: 0.12702903151512146
training loss: 0.12693101167678833
training loss: 0.1269322633743286
training loss: 0.37693125009536743
training loss: 0.1269734799861908
training loss: 0.12694141268730164
training loss: 0.12693502008914948
training loss: 0.3771543502807617
training loss: 0.12696729600429535
training loss: 0.12705835700035095
training loss: 0.12706191837787628
training loss: 0.1273411214351654
training loss: 0.12695850431919098
training loss: 0.12699313461780548
training loss: 0.12727470695972443
training loss: 0.1269485354423523
training loss: 0.126935675740242
training loss: 0.8769276738166809
training loss: 0.12746049463748932
training loss: 0.37700116634368896
training loss: 0.12694980204105377
training loss: 0.12700550258159637
training loss: 0.12693540751934052
training loss: 0.12693452835083008
training loss: 0.1270180493593216
training loss: 0.12694859504699707
training loss: 0.1269618421792984
training loss: 0.12695951759815216
training loss: 0.12696649134159088
training loss: 0.37700486183166504
training loss: 0.37692949175834656
training loss: 0.12696722149848938
training loss: 0.12747927010059357
training loss: 0.12693536281585693
training loss: 0.37695997953414917
training loss: 0.1269558221101761
training loss: 0.12694446742534637
training loss: 0.8770428895950317
training loss: 0.3770962953567505
training loss: 0.12696851789951324
training loss: 0.1270657181739807
training loss: 0.12798157334327698
training loss: 0.6269005537033081
training loss: 0.3769315183162689
training loss: 0.12694643437862396
training loss: 0.1276526153087616
training loss: 0.37696969509124756
training loss: 0.12720520794391632
training loss: 0.12694478034973145
training loss: 0.12736085057258606
training loss: 0.1270192265510559
training loss: 0.3763728737831116
training loss: 0.12694764137268066
training loss: 0.12693549692630768
training loss: 0.12701743841171265
training loss: 0.12726877629756927
training loss: 0.12701140344142914
training loss: 0.12693238258361816
training loss: 0.12694035470485687
training loss: 0.12694983184337616
training loss: 0.34950488805770874
training loss: 0.3768661618232727
training loss: 0.12694993615150452
training loss: 0.12693198025226593
training loss: 0.12693192064762115
training loss: 0.12693271040916443
training loss: 0.12694613635540009
training loss: 0.3770540952682495
training loss: 0.12705159187316895
training loss: 0.12718507647514343
training loss: 0.12711183726787567
training loss: 0.12696529924869537
training loss: 0.1269916594028473
training loss: 0.12695112824440002
training loss: 0.37690457701683044
training loss: 0.12697076797485352
training loss: 0.1273263394832611
training loss: 0.12693867087364197
training loss: 0.12693007290363312
training loss: 0.12693338096141815
training loss: 0.1269337683916092
training loss: 0.37693315744400024
training loss: 0.12695519626140594
training loss: 0.37689998745918274
training loss: 0.1270345151424408
training loss: 0.12751156091690063
training loss: 0.3743549585342407
training loss: 0.12729914486408234
training loss: 0.12693971395492554
training loss: 0.12700709700584412
training loss: 0.1270880103111267
training loss: 0.3769807815551758
training loss: 0.1269514560699463
training loss: 0.37692874670028687
training loss: 0.3769434988498688
training loss: 0.12923061847686768
training loss: 0.37694939970970154
training loss: 0.12695318460464478
training loss: 0.1269841343164444
training loss: 0.1269363909959793
training loss: 0.6267226338386536
training loss: 0.12702441215515137
training loss: 0.1270487755537033
training loss: 0.12721827626228333
training loss: 0.12694263458251953
training loss: 0.37693703174591064
training loss: 0.12694783508777618
training loss: 0.3768990635871887
training loss: 0.12703178822994232
training loss: 0.12692853808403015
training loss: 0.1269511729478836
training loss: 0.3769434094429016
training loss: 0.1270368993282318
training loss: 0.12719930708408356
training loss: 0.1269698441028595
training loss: 0.1270928531885147
training loss: 0.12699131667613983
training loss: 0.6269222497940063
training loss: 0.6269314289093018
training loss: 0.12716113030910492
training loss: 0.1269882768392563
training loss: 0.12695005536079407
training loss: 0.12694674730300903
training loss: 0.12706299126148224
training loss: 0.1269470751285553
training loss: 0.12707634270191193
training loss: 0.1269436776638031
training loss: 0.12695227563381195
training loss: 0.12800860404968262
training loss: 0.1269388049840927
training loss: 0.12700003385543823
training loss: 0.12694105505943298
training loss: 0.12839902937412262
training loss: 0.12696149945259094
training loss: 0.12695056200027466
training loss: 0.1269582211971283
training loss: 0.37693464756011963
training loss: 0.12693342566490173
training loss: 0.12695017457008362
training loss: 0.12697234749794006
training loss: 0.3769667446613312
training loss: 0.3769518733024597
training loss: 0.12697362899780273
training loss: 0.12693312764167786
training loss: 0.12698259949684143
training loss: 0.37693431973457336
training loss: 0.12696710228919983
training loss: 0.37693601846694946
training loss: 0.12695078551769257
training loss: 0.37670692801475525
training loss: 0.1269664466381073
training loss: 0.12695178389549255
training loss: 0.12693390250205994
training loss: 0.12707379460334778
training loss: 0.12693479657173157
training loss: 0.12693579494953156
training loss: 0.12720641493797302
training loss: 0.12694098055362701
training loss: 0.12695638835430145
training loss: 0.12700524926185608
training loss: 0.12702509760856628
training loss: 0.12698478996753693
training loss: 0.12972937524318695
training loss: 0.12701298296451569
training loss: 0.1274588406085968
training loss: 0.12693017721176147
training loss: 0.12693487107753754
training loss: 0.3766609728336334
training loss: 0.1269310861825943
training loss: 0.1269601285457611
training loss: 0.12694132328033447
training loss: 0.12699894607067108
training loss: 0.12700742483139038
training loss: 0.12696585059165955
training loss: 0.12695163488388062
training loss: 0.62764573097229
training loss: 0.3769296407699585
training loss: 0.376943439245224
training loss: 0.1270374059677124
training loss: 0.1270555704832077
training loss: 0.6274102926254272
training loss: 0.3768448233604431
training loss: 0.12701961398124695
training loss: 0.1269490271806717
training loss: 0.1269470453262329
training loss: 0.12705878913402557
training loss: 0.12694741785526276
training loss: 0.1270119696855545
training loss: 0.37693169713020325
training loss: 0.12693527340888977
training loss: 0.3772161602973938
training loss: 0.12693524360656738
training loss: 0.12692975997924805
training loss: 0.12704730033874512
training loss: 0.12695276737213135
training loss: 0.376955509185791
training loss: 0.1269693374633789
training loss: 0.376934677362442
training loss: 0.12703748047351837
training loss: 0.12709654867649078
training loss: 0.37692683935165405
training loss: 0.12695986032485962
training loss: 0.1271807849407196
training loss: 0.12694767117500305
training loss: 0.12706947326660156
training loss: 0.12693054974079132
training loss: 0.12693490087985992
training loss: 0.1270270049571991
training loss: 0.12694211304187775
training loss: 0.12758643925189972
training loss: 0.12702280282974243
training loss: 0.12693053483963013
training loss: 0.3769477605819702
training loss: 0.37099018692970276
training loss: 0.12730008363723755
training loss: 0.1269386112689972
training loss: 0.3770464062690735
training loss: 0.1269364207983017
training loss: 0.3767673671245575
training loss: 0.1430870145559311
training loss: 0.1282670646905899
training loss: 0.12695369124412537
training loss: 0.127583846449852
training loss: 0.1271650493144989
training loss: 0.12693694233894348
training loss: 0.12694968283176422
training loss: 0.12700621783733368
training loss: 0.12693411111831665
training loss: 0.12694726884365082
training loss: 0.12695854902267456
training loss: 0.12703682482242584
training loss: 0.37719401717185974
training loss: 0.12736602127552032
training loss: 0.12696626782417297
training loss: 0.1270352154970169
training loss: 0.12711048126220703
training loss: 0.1288038194179535
training loss: 0.12696756422519684
training loss: 0.3769456446170807
training loss: 0.3769504427909851
training loss: 0.12693914771080017
training loss: 0.12716372311115265
training loss: 0.37694498896598816
training loss: 0.12693876028060913
training loss: 0.37695255875587463
training loss: 0.12693624198436737
training loss: 0.127506285905838
training loss: 0.3785547614097595
training loss: 0.12699198722839355
training loss: 0.3769342601299286
training loss: 0.376945436000824
training loss: 0.1269606202840805
training loss: 0.12693139910697937
training loss: 0.12693919241428375
training loss: 0.12702196836471558
training loss: 0.12694492936134338
training loss: 0.6269292235374451
training loss: 0.12693756818771362
training loss: 0.12701882421970367
training loss: 0.12694185972213745
training loss: 0.12694032490253448
training loss: 0.12700888514518738
training loss: 0.1271812617778778
training loss: 0.12693238258361816
training loss: 0.1269422471523285
training loss: 0.12696385383605957
training loss: 0.12692983448505402
training loss: 0.12711560726165771
training loss: 0.3769358694553375
training loss: 0.37820664048194885
training loss: 0.1269368976354599
training loss: 0.12695755064487457
training loss: 0.12693636119365692
training loss: 0.126990407705307
training loss: 0.12694357335567474
training loss: 0.12707000970840454
training loss: 0.1269286721944809
Average loss at epoch 5: 0.1949906632058568
training loss: 0.1269722878932953
training loss: 0.1272132247686386
training loss: 0.12725044786930084
training loss: 0.37680572271347046
training loss: 0.12694105505943298
training loss: 0.1269586682319641
training loss: 0.37693148851394653
training loss: 0.3769400119781494
training loss: 0.3769627511501312
training loss: 0.12693651020526886
training loss: 0.37689918279647827
training loss: 0.37708333134651184
training loss: 0.1270132064819336
training loss: 0.12700709700584412
training loss: 0.12694554030895233
training loss: 0.1271335482597351
training loss: 0.1269306242465973
training loss: 0.12699753046035767
training loss: 0.12714992463588715
training loss: 0.1269431859254837
training loss: 0.1269369125366211
training loss: 0.12698671221733093
training loss: 0.3771039545536041
training loss: 0.6268057823181152
training loss: 0.12766966223716736
training loss: 0.12695863842964172
training loss: 0.12699484825134277
training loss: 0.12694375216960907
training loss: 0.375516414642334
training loss: 0.127032071352005
training loss: 0.1269305944442749
training loss: 0.12707844376564026
training loss: 0.12694916129112244
training loss: 0.3770880401134491
training loss: 0.12694087624549866
training loss: 0.12693443894386292
training loss: 0.12714096903800964
training loss: 0.12702864408493042
training loss: 0.12692886590957642
training loss: 0.3770333528518677
training loss: 0.12705685198307037
training loss: 0.126944899559021
training loss: 0.1272309273481369
training loss: 0.12738515436649323
training loss: 0.12705251574516296
training loss: 0.12698185443878174
training loss: 0.37695589661598206
training loss: 0.3769509494304657
training loss: 0.12790367007255554
training loss: 0.12696203589439392
training loss: 0.12692870199680328
training loss: 0.1269698143005371
training loss: 0.1269853115081787
training loss: 0.12694579362869263
training loss: 0.12879939377307892
training loss: 0.12695734202861786
training loss: 0.1269405484199524
training loss: 0.12743163108825684
training loss: 0.12699981033802032
training loss: 0.12694412469863892
training loss: 0.12692932784557343
training loss: 0.12704645097255707
training loss: 0.126963272690773
training loss: 0.1269596666097641
training loss: 0.12693822383880615
training loss: 0.12693960964679718
training loss: 0.37701690196990967
training loss: 0.12701000273227692
training loss: 0.1269322633743286
training loss: 0.12698964774608612
training loss: 0.6270360946655273
training loss: 0.1270150989294052
training loss: 0.37692952156066895
training loss: 0.1269589364528656
training loss: 0.3769364356994629
training loss: 0.1270013302564621
training loss: 0.12721124291419983
training loss: 0.12693597376346588
training loss: 0.12693029642105103
training loss: 0.12701815366744995
training loss: 0.12693841755390167
training loss: 0.1270167976617813
training loss: 0.1272163689136505
training loss: 0.12693719565868378
training loss: 0.3769330084323883
training loss: 0.12693339586257935
training loss: 0.126947283744812
training loss: 0.12696890532970428
training loss: 0.12696652114391327
training loss: 0.12694388628005981
training loss: 0.1270909309387207
training loss: 0.12694086134433746
training loss: 0.12737420201301575
training loss: 0.1269540786743164
training loss: 0.3769777715206146
training loss: 0.1269424706697464
training loss: 0.12693385779857635
training loss: 0.1271190494298935
training loss: 0.1269327700138092
training loss: 0.12721268832683563
training loss: 0.1270797848701477
training loss: 0.1269640028476715
