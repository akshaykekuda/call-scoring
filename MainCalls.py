# -*- coding: utf-8 -*-
"""MainCalls

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13U7IVc0TcKMzzszxaQt-jxqM-2UEAvah
"""
import argparse
import datetime
import sys
import os

from DatasetClasses import CallDataset
from torch.utils.data import DataLoader
from gensim.models import Word2Vec
from gensim.models.keyedvectors import Word2VecKeyedVectors
from gensim.models import KeyedVectors
from DataLoader_fns import save_vocab
from TrainModel import TrainModel
from DataLoader_fns import collate
from Inference_fns import *
from PrepareDf import *
from sklearn.model_selection import train_test_split, KFold
from nltk.tokenize import word_tokenize

import numpy as np
import torch
import pandas as pd
import time
np.random.seed(0)
torch.manual_seed(0)

path_to_handscored_p = 'ScoringDetail_viw_all_subscore.p' 
word_embedding_pt = dict(glove='../word_embeddings/glove_word_vectors',
                         w2v='../word_embeddings/custom_w2v_100d',
                         fasttext='../word_embeddings/fasttext_300d.bin')
pd.set_option("display.max_rows", None, "display.max_columns", None)

def _parse_args():
    """
    Command-line arguments to the system.
    :return: the parsed args bundle
    """
    parser = argparse.ArgumentParser(description='MainCalls.py')

    # General system running and configuration options
    parser.add_argument('--model', type=str, default='AllSubScores', help='model to run')
    parser.add_argument('--workgroup', type=str, default='all', help='workgroup of calls to score')
    parser.add_argument('--batch_size', type=int, default=1, help='batch_size')
    parser.add_argument('--epochs', type=int, default=30, help='epochs to run')
    parser.add_argument('--train_samples', type=int, default=3500, help='number of samples for training')
    parser.add_argument('--word_embedding', type=str, default='glove', help='word embedding to use')
    parser.add_argument('--attention', type=str, default='hsan', help='attention mechanism to use' )
    parser.add_argument('--save_path', type=str, default='', help='path to save checkpoints')
    parser.add_argument('--num_heads', type=int, default=4, help='number of attention heads')
    parser.add_argument('--model_size', type=int, default=128, help='model size')
    parser.add_argument('--lr', type=float, default=1e-3, help='learning rate')
    parser.add_argument('--dropout', type=float, default=0.2, help='dropout rate')

    args = parser.parse_args()
    return args


def predict_all_subscores(trainer, dataloader_transcripts_test, test_df):

    scoring_criteria = ['Cross Selling', 'Creates Incentive', 'Product Knowledge', 'Education', 'Processes']
    model = trainer.train_all_subscores_model(scoring_criteria)
    torch.save(model, save_path+"call_encoder_bi_class.model")
    # print('Dev Accuracy for Call Transcripts dataset:')
    # get_metrics(dataloader_transcripts_dev, model, 'Category', type='bi_class')
    # print('Dev Baseline Model Metrics:')
    # predict_baseline_metrics(dev_df, type='bi_class')
    print('Test Metrics for Call Transcripts dataset  is:')
    metrics, pred_df = get_metrics(dataloader_transcripts_test, model, scoring_criteria, type='bi_class')
    plot_roc(scoring_criteria, pred_df)
    pred_df.to_pickle(save_path+'all_subscores_pred_test.p')
    # print('Test Baseline Model Metrics:')
    # predict_baseline_metrics(test_df, type='bi_class')
    return metrics

def predict_product_knowledge(trainer, dataloader_transcripts_test, test_df):

    scoring_criteria = ['Product Knowledge']
    model = trainer.train_prod_knowledge_model(scoring_criteria)
    torch.save(model, save_path+"product_knowledge.model")
    print('Test Metrics for Call Transcripts dataset  is:')
    metrics, pred_df = get_metrics(dataloader_transcripts_test, model, scoring_criteria, type='bi_class')
    plot_roc(scoring_criteria, pred_df)
    pred_df.to_pickle(save_path+'prod_knw_pred_test.p')
    # print('Test Baseline Model Metrics:')
    # predict_baseline_metrics(test_df, type='bi_class')
    return metrics

def predict_cross_selling(trainer, dataloader_transcripts_test, test_df):

    scoring_criteria = ['Cross Selling']
    model = trainer.train_all_subscores_model(scoring_criteria)
    torch.save(model, save_path+"cross_selling.model")
    print('Test Metrics for Call Transcripts dataset  is:')
    metrics, pred_df = get_metrics(dataloader_transcripts_test, model, scoring_criteria, type='bi_class')
    plot_roc(scoring_criteria, pred_df)
    pred_df.to_pickle(save_path+'cross_selling_pred_test.p')
    # print('Test Baseline Model Metrics:')
    # predict_baseline_metrics(test_df, type='bi_class')
    return metrics
    


def predict_overall_category(trainer, dataloader_transcripts_test, test_df):
    model = trainer.train_overall_model()

    torch.save(model, "call_encoder_bi_class.model")
    # print('Dev Accuracy for Call Transcripts dataset:')
    # get_metrics(dataloader_transcripts_dev, model, 'Category', type='bi_class')
    # print('Dev Baseline Model Metrics:')
    # predict_baseline_metrics(dev_df, type='bi_class')
    print('Test Metrics for Call Transcripts dataset  is:')
    metrics, pred_df = get_metrics(dataloader_transcripts_test, model, 'Category', type='bi_class')
    print('Test Baseline Model Metrics:')
    predict_baseline_metrics(test_df, type='bi_class')
    return metrics


def predict_overall_score(trainer, dataloader_transcripts_test, test_df):
    model = trainer.train_linear_regressor()
    torch.save(model, "call_encoder_regressor.model")
    # print('Dev MSE for Call Transcripts dataset:')
    # mse, pred_df = get_metrics(dataloader_transcripts_dev, call_encoder, 'CombinedPercentileScore', type='mse')
    # pred_df.to_csv('overall_score_pred_dev.csv')
    # print('Dev Baseline Model MSE:')
    # predict_baseline_metrics(dev_df, type='mse')
    print('Test MSE for Call Transcripts dataset  is:')
    mse, pred_df = get_metrics(dataloader_transcripts_test, model, 'CombinedPercentileScore', type='mse')
    pred_df.to_csv('overall_score_pred_test.csv')
    print('Test Baseline Model MSE:')
    predict_baseline_metrics(test_df, type='mse')
    return mse


def get_max_len(df):
    def fun(sent):
        return len(sent.split())
    max_trans_len = np.max(df.text.apply(lambda x: len(x.split("\n"))))
    max_sent_len = np.max(df.text.apply(lambda x: max(map(fun, x.split('\n')))))
    return max_trans_len, max_sent_len


def run_cross_validation(train_df, test_df):
    kfold_results = []
    dataset_transcripts_test = CallDataset(test_df)
    for train, dev in kf.split(train_df):
        # train, dev = train_test_split(df_sampled, test_size=0.20)
        t_df = train_df.iloc[train].copy()
        # t_df = t_df.loc[t_df.text.apply(lambda x: len(x.split('\n'))).sort_values().index]
        dev_df = train_df.iloc[dev].copy()
        subscore_dist = t_df.loc[:, ['Greeting', 'Professionalism', 'Confidence',
                    'Cross Selling', 'Retention', 'Creates Incentive', 'Product Knowledge',
                    'Documentation', 'Education', 'Processes']].apply(lambda x: x.value_counts())
        print("Subscore distribution count in Training set\n", subscore_dist)
        dataset_transcripts_train = CallDataset(t_df)
        dataset_transcripts_dev = CallDataset(dev_df)
        max_trans_len, max_sent_len = get_max_len(train_df)
        vocab = dataset_transcripts_train.get_vocab()
        vocab.insert_token('<pad>', 0)
        vocab.insert_token('<UNK>', 0)
        save_vocab(vocab, 'vocab')

        batch_size = args.batch_size

        dataloader_transcripts_train = DataLoader(dataset_transcripts_train, batch_size=batch_size, shuffle=True,
                                                  num_workers=4, collate_fn=collate)
        dataloader_transcripts_dev = DataLoader(dataset_transcripts_dev, batch_size=batch_size, shuffle=False,
                                                num_workers=4, collate_fn=collate)
        dataloader_transcripts_test = DataLoader(dataset_transcripts_test, batch_size=batch_size, shuffle=False,
                                                 num_workers=4, collate_fn=collate)
        # model = Word2Vec.load('custom_w2v_100d')

        model = KeyedVectors.load(word_embedding_pt[args.word_embedding], mmap='r')
        vec_size = model.vector_size
        vocab_size = len(vocab)
        weights_matrix = np.zeros((vocab_size, vec_size))
        i = 0
        for word in vocab.get_itos()[2:]:
            try:
                weights_matrix[i] = model[word]  # model.wv[word] for trained word2vec
            except KeyError:
                weights_matrix[i] = np.random.normal(scale=0.6, size=(vec_size,))
            i += 1
        weights_matrix[0] = np.mean(weights_matrix, axis=0)
        weights_matrix = torch.tensor(weights_matrix)
        trainer = TrainModel(dataloader_transcripts_train, dataloader_transcripts_dev, vocab_size, vec_size,
                             weights_matrix, args, max_trans_len, max_sent_len)
        if args.model == 'Category':
            metrics = predict_overall_category(trainer, dataloader_transcripts_test, test_df)
        elif args.model == 'CombinedPercentileScore':
            mse = predict_overall_score(trainer, dataloader_transcripts_test, test_df)
        elif args.model == 'AllSubScores':
            metrics = predict_all_subscores(trainer, dataloader_transcripts_test, test_df)
        elif args.model == 'Cross_Selling':
            metrics = predict_cross_selling(trainer, dataloader_transcripts_test, test_df)
        elif args.model == 'Product_Knowledge':
            metrics = predict_product_knowledge(trainer, dataloader_transcripts_test, test_df)
        break
    return kfold_results


if __name__ == "__main__":
    args = _parse_args()
    original_stdout = sys.stdout # Save a reference to the original standard output
    os.mkdir('logs/'+args.save_path)
    save_path = 'logs/'+args.save_path+'/'
    log_file = save_path + 'log.txt'

    with open(save_path+'log.txt', 'w') as f:
        sys.stdout = f # Change the standard output to the file we created.
        print("Arguments:", args)
        kf = KFold(n_splits=5, shuffle=True)
        score_df, score_comment_df, q_text = prepare_score_df(
            path_to_handscored_p, workgroup=args.workgroup)
        transcript_score_df = prepare_trancript_score_df(score_df, q_text)
        train_df, test_df = train_test_split(transcript_score_df, test_size=0.15)
        if args.train_samples>0:
            train_df = balance_df(train_df, args.train_samples)
        kfold_results = run_cross_validation(train_df, test_df)
        sys.stdout = original_stdout # Reset the standard output to its original value


    # avg_tuple = [sum(y) / len(y) for y in zip(*kfold_results)]
    # print("Overall accuracy={} Overall F1 score={}".format(avg_tuple[0], avg_tuple[1]))
