# -*- coding: utf-8 -*-
"""MainCalls

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13U7IVc0TcKMzzszxaQt-jxqM-2UEAvah
"""
import argparse, configparser
import datetime
import sys
import os, json
from pathlib import Path
from DatasetClasses import CallDataset, CallDatasetWithFbk, MLMDataSet
from torch.utils.data import DataLoader, random_split
from DataLoader_fns import Collate, CollateDual
from FeedbackComments import FeedbackComments
from gensim.models import Word2Vec
from gensim.models.keyedvectors import Word2VecKeyedVectors
from gensim.models import KeyedVectors
from TrainModel import TrainModel
from Inference_fns import *
from PrepareDf import *
from sklearn.model_selection import train_test_split, KFold, ShuffleSplit
from nltk.tokenize import word_tokenize
from transformers import BertTokenizerFast, DataCollatorForLanguageModeling
import random
from tokenizers import BertWordPieceTokenizer
import numpy as np
import torch
import pandas as pd
from pathlib import Path
import pdb
import time

pd.set_option("display.max_rows", None, "display.max_columns", None)

global vocab
sub_score_categories = ['Cross Selling', 'Creates Incentive', 'Education', 'Processes', 'Product Knowledge', 'Greeting', 'Professionalism', 'Confidence',  'Retention',
                        'Documentation']


def _parse_args():
    """
    Command-line arguments to the system.
    :return: the parsed args bundle
    """
    parser = argparse.ArgumentParser(description='MainCalls.py')

    # General system running and configuration options
    parser.add_argument('--subscore', type=str, default='all', help='model to run')
    parser.add_argument('--workgroup', type=str, default='all', help='workgroup of calls to score')
    parser.add_argument('--batch_size', type=int, default=4, help='batch_size')
    parser.add_argument('--epochs', type=int, default=1, help='epochs to run')
    parser.add_argument('--train_samples', type=int, default=50, help='number of samples for training')
    parser.add_argument('--word_embedding', type=str, default='glove', help='word embedding to use')
    parser.add_argument('--model', type=str, default='hs2an', help='attention mechanism to use')
    parser.add_argument('--save_path', type=str, default='logs/test/', help='path to save checkpoints')
    parser.add_argument('--word_nh', type=int, default=1, help='number of attention heads for word attn')
    parser.add_argument('--sent_nh', type=int, default=1, help='number of attention heads for sent attn')
    parser.add_argument('--model_size', type=int, default=64, help='model size')
    parser.add_argument('--lr', type=float, default=1e-4, help='learning rate')
    parser.add_argument('--dropout', type=float, default=0.2, help='dropout rate')
    parser.add_argument('--trans_path', type=str, default='transcriptions/text_only/', help='link to transcripts')
    parser.add_argument('--test_path', type=str, default='transcriptions/test_transcriptions/', help='link to test transcripts')
    parser.add_argument('--trans2_path', type=str, help='link to transcripts')
    parser.add_argument('--test2_path', type=str, help='link to test transcripts')
    parser.add_argument('--device', type=str, default='cpu', help='device to use')
    parser.add_argument('--loss', type=str, default='bce', help='optimizer to use')
    parser.add_argument('--k', type=int, default=20, help='number of top comments to use')
    parser.add_argument('--num_workers', type=int, default=0, help='number of workers for the dataset')
    parser.add_argument("--save_model", default=False, action="store_true")
    parser.add_argument("--use_feedback", default=False, action="store_true")
    parser.add_argument("--new_data", default=False, action="store_true")
    parser.add_argument("--num_layers", default=1, type=int, help="num of layers of sentence level self attention")
    parser.add_argument("--word_nlayers", default=1, type=int, help="num of layers of word level self attention")
    parser.add_argument("--reg", default=1e-5, type=float, help="l2 regularization")
    parser.add_argument("--acum_step", default=1, type=int, help="grad accumulation steps")
    parser.add_argument("--tok_path", default='sa_tokenizer/', type=str, help="path to trained tokenizer")
    parser.add_argument("--doc2vec_pt", default='../word_embeddings/trained_doc2vec', type=str, help="path to trained doc2vec model")
    parser.add_argument("--config", default='default', type=str, help="configuration to use")
    parser.add_argument("--fold", default=1, type=int, help="folds to run")
    args = parser.parse_args()
    return args


def predict_scores(trainer, dataloader_transcripts_test):
    if args.loss == 'cel':
        model = trainer.train_cel_model()
    elif args.loss == 'bce':
        model = trainer.train_bce_model()
    elif args.loss == 'mse':
        model = trainer.train_linear_regressor()
    else:
        raise ValueError("Invalid loss function {}".format(args.loss))
    if args.save_model:
        torch.save(model, args.save_path+"call_score.model")
    print('Test Metrics for Call Transcripts dataset  is:')
    metrics, pred_df = get_metrics(dataloader_transcripts_test, model, scoring_criteria, loss=args.loss)
    plot_roc(scoring_criteria, pred_df, args.save_path + 'fold_'+ str(trainer.fold) +'_auc.png')
    pred_df.to_pickle(args.save_path+'fold_' + str(trainer.fold)+'_call_score_test.p')
    return metrics


def predict_scores_mtl(trainer, dataloader_transcripts_test):
    model = trainer.train_mtl_model()
    if args.save_model:
        torch.save(model, args.save_path+"call_score_mtl.model")
    print('Test Metrics  for Call Transcripts dataset  is:')
    metrics, pred_df = get_metrics(dataloader_transcripts_test, model, scoring_criteria, loss=args.loss)
    plot_roc(scoring_criteria, pred_df, args.save_path+'auc.png')
    pred_df.to_pickle(args.save_path+'call_score_mtl_test.p')
    return metrics


def get_max_len(df):
    def fun(sent):
        return len(sent.split())
    max_trans_len = np.max(df.text.apply(lambda x: len(x.split("\n"))))
    max_sent_len = np.max(df.text.apply(lambda x: max(map(fun, x.split('\n')))))
    return max_trans_len, max_sent_len


def get_dataset(type, dataset_dir, df, scoring_criteria):
    if type == 'train':
        if args.use_feedback:
            path = dataset_dir + 'train_fbck'
        else:
            path = dataset_dir + 'train'

    elif type == 'dev':
        path = dataset_dir + 'dev'
    else:
        path = dataset_dir + 'test'

    if Path(path).is_file():
        with open(path, 'rb') as f:
            ds = pickle.load(f)
    else:
        if args.use_feedback:
            ds = CallDatasetWithFbk(df, scoring_criteria)
        else:
            ds = CallDataset(df, scoring_criteria)

        with open(path, 'wb') as f:
            pickle.dump(ds, f)
    return ds


def run_cross_validation(train_df, test_df):
    # kf = KFold(n_splits=2, shuffle=True)
    print("seed = {}".format(seed))
    kf = ShuffleSplit(n_splits=args.fold, test_size=0.15, random_state=seed)
    kfold_results = []
    fold = 0
    for train, dev in kf.split(train_df):
        t_df = train_df.iloc[train].copy()
        dev_df = train_df.iloc[dev].copy()
        subscore_dist = t_df.loc[:, scoring_criteria].apply(lambda x: x.value_counts())
        print("Subscore distribution count in Training set\n", subscore_dist)
        subscore_dist = dev_df.loc[:, scoring_criteria].apply(lambda x: x.value_counts())
        print("Subscore distribution count in Dev set\n", subscore_dist)
        dataset_transcripts_train = CallDataset(t_df, scoring_criteria)
        dataset_transcripts_dev = CallDataset(dev_df, scoring_criteria)
        dataset_transcripts_test = CallDataset(test_df, scoring_criteria)
        dataset_transcripts_train.save_vocab('call_vocab')
        vocab = torch.load('call_vocab')
        batch_size = args.batch_size
        c = CollateDual(vocab, args.device, dataset_transcripts_train.dual)
                            
        dataloader_transcripts_train = DataLoader(dataset_transcripts_train, batch_size=batch_size, shuffle=True,
                                                  num_workers=args.num_workers, collate_fn = c.collate)
        dataloader_transcripts_dev = DataLoader(dataset_transcripts_dev, batch_size=batch_size, shuffle=False,
                                                num_workers=args.num_workers,collate_fn = c.collate)
        dataloader_transcripts_test = DataLoader(dataset_transcripts_test, batch_size=batch_size, shuffle=False,
                                                 num_workers=args.num_workers, collate_fn = c.collate)
        # pdb.set_trace()
        embedding_model = KeyedVectors.load(word_embedding_pt[args.word_embedding], mmap='r')
        vocab_size = len(vocab)
        embedding_size = embedding_model.vector_size
        weights_matrix = np.zeros((vocab_size, embedding_size))
        i = 2 # ignore <UNK> and <pad> tokens
        for word in vocab.get_itos()[2:]:
            try:
                weights_matrix[i] = embedding_model[word]  # model.wv[word] for trained word2vec
            except KeyError:
                weights_matrix[i] = np.random.normal(scale=0.6, size=(embedding_size,))
            i += 1
        weights_matrix[0] = np.mean(weights_matrix, axis=0)
        weights_matrix = torch.tensor(weights_matrix)
        trainer = TrainModel(dataloader_transcripts_train, dataloader_transcripts_dev, vocab_size, embedding_size,
                             weights_matrix, args, max_trans_len, max_sent_len, scoring_criteria, fold)
        if args.use_feedback:
            metrics = predict_scores_mtl(trainer, dataloader_transcripts_test)
        else:
            metrics = predict_scores(trainer, dataloader_transcripts_test)
        fold+=1
        kfold_results.append(metrics)
    return kfold_results


def run_cross_validation_mlm(tokenizer):
    print("training mlm")

    paths = [str(x) for x in Path(args.trans_path).glob("**/*.txt")]
    print(args.train_samples)
    if args.train_samples>0:
        mini_paths = random.sample(paths, args.train_samples)
    else:
        mini_paths = paths
    mlm_ds = MLMDataSet(mini_paths, tokenizer)
    train_size = int(0.8 * len(mlm_ds))
    test_size = len(mlm_ds) - train_size
    train_dataset, dev_dataset = random_split(mlm_ds, [train_size, test_size])
    all_test_paths = [str(x) for x in Path(args.test_path).glob("**/*.txt")]
    test_paths = random.sample(all_test_paths, args.train_samples//10)
    test_dataset = MLMDataSet(test_paths, tokenizer)
    collator = DataCollatorForLanguageModeling(tokenizer)
    dataloader_transcripts_train = DataLoader(train_dataset, batch_size=4, shuffle=True,
                                              collate_fn=collator.torch_call)
    dataloader_transcripts_dev = DataLoader(dev_dataset, batch_size=4, shuffle=True,
                                              collate_fn=collator.torch_call)
    dataloader_transcripts_test = DataLoader(test_dataset, batch_size=1, shuffle=True,
                                             collate_fn=collator.torch_call)
    fold = 0
    trainer = TrainModel(dataloader_transcripts_train, dataloader_transcripts_dev, None, None,
                         None, args, max_trans_len, max_sent_len, None, fold)
    trainer.train_mlm_model(tokenizer)
    model = torch.load(args.save_path + 'fold_' + str(fold) + '_best_mlm_model')
    test_error, pred_df = get_mlm_metrics(dataloader_transcripts_test, model, tokenizer)
    pred_df.to_pickle(args.save_path+'fold_' + str(trainer.fold)+'_mlm_test.p')
    print("Test set loss = {}".format(test_error))


def train_tokenizer(file_path, save_path):
    print("training tokenizer")
    paths = [str(x) for x in Path(file_path).glob("**/*.txt")]
    tokenizer = BertWordPieceTokenizer()
    tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[
        "[CLS]",
        "[PAD]",
        "[UNK]",
        "[MASK]",
        "[SEP]",
    ], show_progress=True)
    os.mkdir(save_path)
    tokenizer.save_model(save_path)
    with open(os.path.join(save_path, "config.json"), "w") as f:
        tokenizer_cfg = {
            "do_lower_case": True,
            "unk_token": "[UNK]",
            "sep_token": "[SEP]",
            "pad_token": "[PAD]",
            "cls_token": "[CLS]",
            "mask_token": "[MASK]",
            "model_max_length": 512,
            "max_len": 512,
        }
        json.dump(tokenizer_cfg, f)

def parse_config(path="config.ini", configuration='default'):
    config_obj = configparser.ConfigParser()
    config_obj.read(path)
    return config_obj[configuration]


if __name__ == "__main__":
    args = _parse_args()
    args.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    config = parse_config(configuration=args.config)
    path_to_handscored_p = config["path_to_handscored_p"]

    word_embedding_pt = dict(glove=config["glove_path"],
                            w2v='/mnt/transcriber/word_embeddings/custom_w2v_100d',
                            fasttext='../word_embeddings/fasttext_300d.bin')
    dataset_dir = config["dataset_dir"]
    seed = int(config["seed"])
    max_trans_len, max_sent_len = int(config["max_trans_len"]), int(config["max_sent_len"])

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    print("Arguments:", args)
    if args.model == 'mlm':
        if not os.path.isdir(args.tok_path):
            train_tokenizer(args.trans_path, args.tok_path)
        tokenizer = BertTokenizerFast.from_pretrained(args.tok_path, add_special_tokens=True)
        run_cross_validation_mlm(tokenizer)
    else:
        train_ds_path = dataset_dir + 'train'
        test_ds_path = dataset_dir + 'test'
        if args.new_data:
            print("creating the dataframes")
            score_df, q_text = prepare_score_df(
                path_to_handscored_p, workgroup=args.workgroup)
            train_df = prepare_trancript_score_df(score_df, q_text, args.trans_path, args.trans2_path)
            with open(train_ds_path, 'wb') as f:
                pickle.dump(train_df, f)
            test_df = prepare_trancript_score_df(score_df, q_text, args.test_path, args.test2_path)
            with open(test_ds_path, 'wb') as f:
                pickle.dump(test_df, f)
            # pdb.set_trace()
        else:
            print("Getting dataset from saved files")
            with open(train_ds_path, 'rb') as f:
                train_df = pickle.load(f)
            with open(test_ds_path, 'rb') as f:
                test_df = pickle.load(f)

        if args.subscore == 'all':
            scoring_criteria = sub_score_categories
        elif args.subscore == 'best':
            scoring_criteria = sub_score_categories[:4]
        else:
            scoring_criteria = [args.subscore]
        if args.use_feedback:
            comment_obj = FeedbackComments(train_df, args.k)
            top_k_comments = comment_obj.extract_top_k_comments(scoring_criteria)
            train_df = comment_obj.df
        if args.train_samples > 0:
            train_df = balance_df(train_df, args.train_samples)

        subscore_dist = test_df.loc[:, scoring_criteria].apply(lambda x: x.value_counts())
        print("Subscore distribution count in Test set\n", subscore_dist)
        kfold_results = run_cross_validation(train_df, test_df)

    # avg_tuple = [sum(y) / len(y) for y in zip(*kfold_results)]
    # print("Overall accuracy={} Overall F1 score={}".format(avg_tuple[0], avg_tuple[1]))
