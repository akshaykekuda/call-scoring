# -*- coding: utf-8 -*-
"""MainCalls

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13U7IVc0TcKMzzszxaQt-jxqM-2UEAvah
"""

from DatasetClasses import CallDataset
from torch.utils.data import DataLoader
from gensim.models import Word2Vec
from gensim.models.keyedvectors import Word2VecKeyedVectors
from DataLoader_fns import save_vocab
from TrainModel import train_model
from DataLoader_fns import collate
from Inference_fns import get_metrics
from PrepareDf import prepare_df
from sklearn.model_selection import train_test_split, KFold
from Models import GRUAttention, LSTMAttention
from nltk.tokenize import word_tokenize

import numpy as np
import torch
import pandas as pd
import time

np.random.seed(0)
torch.manual_seed(0)
kf = KFold(n_splits=5, shuffle=True)
df_sampled = prepare_df()

kfold_results = []
for train,dev in kf.split(df_sampled):
    # train, dev = train_test_split(df_sampled, test_size=0.20)
    filenames_train = df_sampled.iloc[train].file_name
    classifications_train = df_sampled.iloc[train].Category
    filenames_dev = df_sampled.iloc[dev].file_name
    classifications_dev = df_sampled.iloc[dev].Category
    #filenames_test = df.loc[temp].file_name
    #classifications_test = df.loc[temp].Category

    dataset_transcripts_train = CallDataset(filenames_train, classifications_train)
    dataset_transcripts_dev = CallDataset(filenames_dev, classifications_dev)
    #dataset_transcripts_test = CallDataset(filenames_test, classifications_test)

    vocab = dataset_transcripts_train.get_vocab()
    save_vocab(vocab, 'vocab')

    batch_size = 1

    dataloader_transcripts_train = DataLoader(dataset_transcripts_train, batch_size=batch_size, shuffle=True, 
                                  num_workers=0, collate_fn = collate)
    dataloader_transcripts_dev = DataLoader(dataset_transcripts_dev, batch_size=batch_size, shuffle=True, 
                                  num_workers=0, collate_fn = collate)
    #dataloader_transcripts_test = DataLoader(dataset_transcripts_test, batch_size=batch_size, shuffle=True, 
    #                              num_workers=0, collate_fn = collate)
    # model = Word2Vec.load('custom_w2v_100d')
    model = Word2VecKeyedVectors.load_word2vec_format('glove.w2v.txt')
    vec_size = model.vector_size
    vocab_size = len(vocab)

    weights_matrix = np.zeros((vocab_size, vec_size))
    i = 0
    for word in vocab.itos:
      try:
        weights_matrix[i] = model[word] #model.wv[word] for trained word2vec
      except KeyError:
        weights_matrix[i] = np.random.normal(scale=0.6, size=(vec_size, ))
      i+=1
      
    weights_matrix = torch.tensor(weights_matrix)

    encoder_output_size = 128
    encoder = LSTMAttention(vocab_size, vec_size, encoder_output_size, weights_matrix)

    trained_encoder = train_model(dataloader_transcripts_train, encoder)
    torch.save(trained_encoder, "encoder_calls.model")
    test_metrics = get_metrics(dataloader_transcripts_dev, trained_encoder)
    print('Dev accuracy for Call Transcripts dataset is {}'.format(test_metrics['accuracy']))
    print('Dev F1 for Call Transcripts dataset is {}'.format(test_metrics['f1']))
    print(test_metrics['clr'])
    kfold_results.append((test_metrics['accuracy'], test_metrics['f1']))
    break

avg_tuple = [sum(y) / len(y) for y in zip(*kfold_results)]
print("Overall accuracy={} Overall F1 score={}".format(avg_tuple[0], avg_tuple[1]))
