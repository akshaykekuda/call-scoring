# -*- coding: utf-8 -*-
"""TrainModel

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JkW85J7ZhjzUcKk7sSf8mSfFK6svNtND
"""

from __future__ import unicode_literals, print_function, division
from matplotlib import pyplot as plt
from tqdm import tqdm
from Models import *
from Inference_fns import get_metrics
from sklearn.utils import class_weight
from torch.optim.lr_scheduler import MultiStepLR
import torch.optim as optim

class TrainModel:
    def __init__(self, dataloader_train, dataloader_dev, vocab_size, vec_size, weights_matrix, args, max_trans_len,
                 max_sent_len):
        self.dataloader_train = dataloader_train
        self.dataloader_dev = dataloader_dev
        self.vocab_size = vocab_size
        self.vec_size = vec_size
        self.weights_matrix = weights_matrix
        self.args = args
        self.max_trans_len = max_trans_len
        self.max_sent_len = max_sent_len

    def train(self):
        raise "To be Implemented"


    def get_model(self, num_classes):
        if self.args.attention == 'baseline':
            encoder = EncoderRNN(self.vocab_size, self.vec_size, self.args.model_size, self.weights_matrix,
                                 self.args.dropout, num_classes)
        elif self.args.attention == 'gru_attention':
            encoder = GRUAttention(self.vocab_size, self.vec_size, self.args.model_size, self.weights_matrix,
                                   self.args.dropout, num_classes)
        elif self.args.attention == 'han':
            encoder = HAN(self.vocab_size, self.vec_size, self.args.model_size, self.weights_matrix, self.args.dropout,
                          num_classes)
        elif self.args.attention == 'hsan':
            encoder = HSAN(self.vocab_size, self.vec_size, self.args.model_size, self.weights_matrix,
                           self.max_trans_len, self.max_sent_len, self.args.num_heads, self.args.dropout, num_classes)
        elif self.args.attention == 'hs2an':
            encoder = HS2AN(self.vocab_size, self.vec_size, self.args.model_size, self.weights_matrix,
                            self.max_trans_len, self.max_sent_len, self.args.num_heads, self.args.dropout, num_classes)
        else:
            raise ValueError("Invalid Attention Model argument")

        if self.args.optim == 'mse':
            fcn = FCN_ReLu(2 * self.args.model_size, num_classes, self.args.dropout)
        elif self.args.optim == 'cel' or self.args.optim == 'bce':
            fcn = FCN_Tanh(2 * self.args.model_size, num_classes, self.args.dropout)
        else:
            raise ValueError("Invalid Optimizer argument")
        model = EncoderFCN(encoder, fcn)
        return model

    def get_class_weights(self, scoring_criteria):
        x = [batch['scores'] for batch in self.dataloader_train]
        arr = []
        pos_wt = []
        for b in x:
            arr.extend(b)
        for sub_category in scoring_criteria:
            y_train = [sample[sub_category].item() for sample in arr]
            class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train),
                                                              y=y_train)
            pos_wt.append(class_weights)
        positive_weights = torch.tensor(pos_wt, dtype=torch.float)
        return positive_weights

    def train_biclass_model(self, scoring_criterion):
        # x = [batch['scores'] for batch in self.dataloader_train]
        # arr = []
        # for b in x:
        #     arr.extend(b)
        # y_train = [sample['Product Knowledge'].item() for sample in arr]
        # class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train),
        #                                                       y=y_train)
        # class_weights = torch.tensor(class_weights, dtype=torch.float)
        class_weights = self.get_class_weights(scoring_criterion).squeeze()
        model = self.get_model(num_classes=2)
        loss_fn = nn.CrossEntropyLoss(weight=class_weights)
        model_optimizer = optim.Adam(model.parameters(), lr=self.args.lr)
        scheduler = MultiStepLR(model_optimizer, milestones=[10, 20], gamma=0.1)
        model = self.train_model(self.args.epochs, model, loss_fn, model_optimizer, scheduler,
                                 scoring_criterion=scoring_criterion)
        return model

    def train_multi_label_model(self, scoring_criteria):
        # x = [batch['scores'] for batch in self.dataloader_train]
        # arr = []
        # pos_wt = []
        # for b in x:
        #     arr.extend(b)
        # for sub_category in scoring_criteria:
        #     y_train = [sample[sub_category].item() for sample in arr]
        #     class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train),
        #                                                       y=y_train)
        #     pos_wt.append(class_weights[1])
        # positive_weights = torch.tensor(pos_wt, dtype=torch.float)

        class_weights = self.get_class_weights(scoring_criteria)
        positive_weights = class_weights[:, 1]
        model = self.get_model(num_classes=len(scoring_criteria))
        loss_fn = nn.BCEWithLogitsLoss(pos_weight=positive_weights)
        model_optimizer = optim.Adam(model.parameters(), lr=self.args.lr)
        scheduler = MultiStepLR(model_optimizer, milestones=[10, 20], gamma=0.1)
        epochs = self.args.epochs
        model = self.train_model(epochs, model, loss_fn, model_optimizer, scheduler,
                                 scoring_criterion=scoring_criteria)
        return model

    def train_linear_regressor(self, scoring_criteria):
        # for p in model.parameters():
        #     if p.dim() > 1:
        #         nn.init.xavier_uniform_(p)

        model = self.get_model(num_classes=len(scoring_criteria))
        loss_fn = nn.MSELoss()
        model_optimizer = optim.Adam(model.parameters(), lr=self.args.lr)
        scheduler = MultiStepLR(model_optimizer, milestones=[10, 20], gamma=0.1)
        epochs = self.args.epochs
        model = self.train_model(epochs, model, loss_fn, model_optimizer, scheduler,
                                 scoring_criterion=scoring_criteria)
        return model

    def train_model(self, epochs, model, loss_fn, model_optimizer, scheduler, scoring_criterion):
        print("inside train model")
        train_acc = []
        dev_acc = []
        model = model.to(self.args.device)
        model.train()
        loss_arr = []
        print(model)
        print(loss_fn)
        print(model_optimizer)
        for n in range(epochs):
            epoch_loss = 0
            for batch in tqdm(self.dataloader_train):
                loss = 0
                output, scores = model(batch['indices'], batch['lens'], batch['trans_pos_indices'],
                                         batch['word_pos_indices'])
                if self.args.optim=='mse':
                    # put device in gpu may have to be modified
                    target = torch.tensor([sample[scoring_criterion] for sample in batch['scores']],
                                          dtype=torch.float, device=self.args.device).view(-1, 1)
                elif self.args.optim == 'bce':
                    target = torch.tensor([sample[scoring_criterion] for sample in batch['scores']],
                                          dtype=torch.float, device=self.args.device)
                elif self.args.optim == 'cel':
                    target = torch.tensor([sample[scoring_criterion] for sample in batch['scores']],
                                          dtype=torch.long, device=self.args.device).squeeze(dim=-1)
                else:
                    raise "invalid optimizer"
                loss += loss_fn(output, target)
                model_optimizer.zero_grad()
                epoch_loss += loss.detach().item()
                loss.backward()
                model_optimizer.step()
            scheduler.step()
            avg_epoch_loss = epoch_loss / len(self.dataloader_train)
            print("Average loss at epoch {}: {}".format(n, avg_epoch_loss))
            loss_arr.append(avg_epoch_loss)
            if n % 5 == 4 or n == epochs-1:
                print("Training metric at end of epoch {}:".format(n))
                train_metrics, _ = get_metrics(self.dataloader_train, model, scoring_criterion, self.args.optim)
                print("Dev metric at end of epoch {}:".format(n))
                dev_metrics, _ = get_metrics(self.dataloader_dev, model, scoring_criterion, self.args.optim)
                train_acc.append(train_metrics)
                dev_acc.append(dev_metrics)
        print("Epoch Losses:", loss_arr)
        plt.plot(loss_arr)
        plt.show()
        plt.savefig(self.args.save_path + 'loss.png')
        print("Training Evaluation Metrics: ", train_acc)
        print("Dev Evaluation Metrics: ", dev_acc)
        return model
