# -*- coding: utf-8 -*-
"""TrainModel

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JkW85J7ZhjzUcKk7sSf8mSfFK6svNtND
"""

from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random

import torch
import torch.nn as nn
from torch import optim
import torch.nn.functional as F
from matplotlib import pyplot as plt
from tqdm import tqdm

from Inference_fns import get_metrics

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def train_model(dataloader_train, encoder):

    criterion = nn.CrossEntropyLoss()
    parameters = filter(lambda p: p.requires_grad, encoder.parameters())
    encoder_optimizer = optim.Adam(parameters, lr=0.0001)
    # encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)
    epochs = 30
    encoder.train()
    for n in range(epochs):
        epoch_loss = 0
        for batch in tqdm(dataloader_train):
            loss = 0
            # print(batch)
            try:
                output, scores = encoder(batch['indices'])
            except:
                print(batch)
                continue
            target = batch['category']
            loss += criterion(output, target)
            encoder_optimizer.zero_grad()
            epoch_loss += loss.detach().item()
            loss.backward()
            # print("training loss: {}".format(loss))
            encoder_optimizer.step()
        print("Average loss at epoch {}: {}".format(n, epoch_loss/len(dataloader_train)))
        if n%10 ==0:
            metrics = get_metrics(dataloader_train, encoder)
            print("Training accuracy at end of epoch {}: {}".format(n, metrics['accuracy']))
            print(metrics['clr'])

    return encoder
