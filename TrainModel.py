# -*- coding: utf-8 -*-
"""TrainModel

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JkW85J7ZhjzUcKk7sSf8mSfFK6svNtND
"""

from __future__ import unicode_literals, print_function, division
from io import open
import unicodedata
import string
import re
import random

import numpy as np
# from matplotlib import pyplot as plt
import torch
from tqdm import tqdm
from Models import *

from Inference_fns import get_metrics, get_regression_metrics
from sklearn.utils import class_weight
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class TrainYelpModel():
    def __init__(self, dataloader_train, dataloader_dev, vocab_size, vec_size, weights_matrix):
        self.dataloader_train = dataloader_train
        self.dataloader_dev = dataloader_dev
        self.vocab_size = vocab_size
        self.vec_size = vec_size
        self.weights_matrix = weights_matrix

    def train_model(self):
        encoder_output_size = 32
        encoder = EncoderRNN(self.vocab_size, self.vec_size, encoder_output_size, self.weights_matrix)
        classifier = BinaryClassifier(encoder_output_size)

        criterion = nn.CrossEntropyLoss()

        encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.001)
        classifier_optimizer = optim.Adam(classifier.parameters(), lr=0.001)

        epochs = 1
        for n in range(epochs):
            epoch_loss = 0
            for batch in tqdm(self.dataloader_train):
                encoder.zero_grad()
                classifier.zero_grad()
                loss = 0

                output, hidden = encoder(batch['indices'])
                output = output[:, -1, :]

                output = classifier(output)
                target = batch['category']

                loss += criterion(output, target)
                epoch_loss += loss.detach().item()
                loss.backward()

                encoder_optimizer.step()
                classifier_optimizer.step()

            print("Average loss at epoch {}: {}".format(n, epoch_loss / len(self.dataloader_train)))
            acc = get_accuracy(self.dataloader_train, encoder, classifier)
            print("Average accuracy at epoch {}: {}".format(n, acc))

        return encoder, classifier

class TrainModel():
    def __init__(self, dataloader_train, dataloader_dev, vocab_size, vec_size, weights_matrix):
        self.dataloader_train = dataloader_train
        self.dataloader_dev = dataloader_dev
        self.vocab_size = vocab_size
        self.vec_size = vec_size
        self.weights_matrix = weights_matrix

    def train_cross_selling_model(self):
        x = [batch['subscores'] for batch in self.dataloader_train]
        arr = []
        for b in x:
            arr.extend(b)
        y_train = [sample['Cross Selling'].item() for sample in arr]
        class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
        # class_weights = [0.8, 0.2]
        class_weights = torch.tensor(class_weights, dtype=torch.float)
        hidden_size =64
        encoder = HAN(self.vocab_size, self.vec_size, hidden_size, self.weights_matrix)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)
        epochs = 20
        model = self.train_model(epochs, encoder, criterion, encoder_optimizer,
                                 scoring_criterion="Cross Selling", type='bi_class')
        return model

    def train_overall_model(self):
        x = [batch['scores'] for batch in self.dataloader_train]
        arr = []
        for b in x:
            arr.extend(b)
        y_train = [sample['Category'].item() for sample in arr]
        class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)
        # class_weights = [0.8, 0.2]
        class_weights = torch.tensor(class_weights, dtype=torch.float)
        hidden_size =64
        encoder = HAN(self.vocab_size, self.vec_size, hidden_size, self.weights_matrix)
        criterion = nn.CrossEntropyLoss(weight=class_weights)
        encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001)
        epochs = 10
        model = self.train_model(epochs, encoder, criterion, encoder_optimizer,
                                 scoring_criterion="Category", type='bi_class')
        return model

    def train_linear_regressor(self):
        hidden_size = 64
        encoder = HAN_Regression(self.vocab_size, self.vec_size, hidden_size, self.weights_matrix)
        for p in encoder.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
        criterion = nn.MSELoss()
        encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.01)
        scheduler = MultiStepLR(optimizer, milestones=[10, 30], gamma=0.1)
        epochs = 50
        model = self.train_model(epochs, encoder, criterion, encoder_optimizer, scoring_criterion="CombinedPercentileScore", type='mse')
        return model

    def train_model(self, epochs, encoder, criterion, encoder_optimizer, scoring_criterion, type):
        print("inside train model")
        print(len(self.dataloader_train))
        train_acc = []
        dev_acc = []
        encoder.train()
        for n in range(epochs):
            epoch_loss = 0
            for batch in tqdm(self.dataloader_train):
                loss = 0
                try:
                    output, scores = encoder(batch['indices'])
                except IndexError:
                    print(batch['indices'])
                if type == 'mse':
                    target = torch.tensor([sample[scoring_criterion] for sample in batch['scores']], dtype=torch.float).view(-1, 1)
                elif type =='bi_class':
                    target = torch.tensor([sample[scoring_criterion] for sample in batch['scores']], dtype=torch.long)
                loss += criterion(output, target)
                encoder_optimizer.zero_grad()
                epoch_loss += loss.detach().item()
                loss.backward()
                encoder_optimizer.step()
            print("Average loss at epoch {}: {}".format(n, epoch_loss/len(self.dataloader_train)))
            if n%5 ==4:
                print("Training MSE at end of epoch {}:".format(n))
                get_metrics(self.dataloader_train, encoder, scoring_criterion, type)
                print("Dev MSE at end of epoch {}:".format(n))
                get_metrics(self.dataloader_dev, encoder, scoring_criterion, type)
        print(train_acc, dev_acc)
        return encoder

