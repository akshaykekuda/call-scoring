# -*- coding: utf-8 -*-
"""MainCalls

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13U7IVc0TcKMzzszxaQt-jxqM-2UEAvah
"""

from DatasetClasses import CallDataset
from torch.utils.data import DataLoader
from gensim.models import Word2Vec
from gensim.models.keyedvectors import Word2VecKeyedVectors
from DataLoader_fns import save_vocab
from TrainModel import train_model
from DataLoader_fns import collate
from Inference_fns import get_metrics
from PrepareDf import prepare_df
from sklearn.model_selection import train_test_split, KFold
from Models import GRUAttention
from nltk.tokenize import word_tokenize
from gensim.models.fasttext import load_facebook_vectors

from gensim.test.utils import datapath

import numpy as np
import torch
import pandas as pd
import time

np.random.seed(0)

kf = KFold(n_splits=2)
df_sampled = prepare_df()
kfold_results = []
for train,dev in kf.split(df_sampled):
    # train, dev = train_test_split(df_sampled, test_size=0.20)
    filenames_train = df_sampled.iloc[train].file_name
    classifications_train = df_sampled.iloc[train].Category
    filenames_dev = df_sampled.iloc[dev].file_name
    classifications_dev = df_sampled.iloc[dev].Category
    #filenames_test = df.loc[temp].file_name
    #classifications_test = df.loc[temp].Category

    dataset_transcripts_train = CallDataset(filenames_train, classifications_train)
    dataset_transcripts_dev = CallDataset(filenames_dev, classifications_dev)
    #dataset_transcripts_test = CallDataset(filenames_test, classifications_test)

    vocab = dataset_transcripts_train.get_vocab()
    save_vocab(vocab, 'vocab')

    batch_size = 8

    dataloader_transcripts_train = DataLoader(dataset_transcripts_train, batch_size=batch_size, shuffle=True, 
                                  num_workers=0, collate_fn = collate)
    dataloader_transcripts_dev = DataLoader(dataset_transcripts_dev, batch_size=batch_size, shuffle=True, 
                                  num_workers=0, collate_fn = collate)
    #dataloader_transcripts_test = DataLoader(dataset_transcripts_test, batch_size=batch_size, shuffle=True, 
    #                              num_workers=0, collate_fn = collate)
    # model = Word2Vec.load('custom_w2v_100d')
    #model = Word2VecKeyedVectors.load_word2vec_format('glove.w2v.txt')
    #vec_size = model.vector_size

    fb_model = load_facebook_vectors('fasttext_300d.bin')
    vec_size = fb_model.vector_size
    vocab_size = len(vocab)
    """
    window_size = 40
    min_word = 5
    down_sampling = 1e-2


    ft_model = FastText(sentences = vocab.itos,
                      vector_size=vec_size,
                      window=window_size,
                      min_count=min_word,
                      sample=down_sampling,
                      sg=1,
                      epochs=100)
    """
    
    i = 0
    weights_matrix = np.zeros((vocab_size, vec_size))
    for word in vocab.itos:
      weights_matrix[i] = fb_model[word] #model.wv[word] for trained word2vec
      i+=1
      
    weights_matrix = torch.tensor(weights_matrix)

    encoder_output_size = 64
    encoder = GRUAttention(vocab_size, vec_size, encoder_output_size, weights_matrix)

    encoder = train_model(dataloader_transcripts_train, encoder)

    test_metrics = get_metrics(dataloader_transcripts_dev, encoder)
    print('Dev accuracy for Call Transcripts dataset is {}'.format(test_metrics['accuracy']))
    print('Dev F1 for Call Transcripts dataset is {}'.format(test_metrics['f1']))
    print(test_metrics['clr'])
    kfold_results.append((test_metrics['accuracy'], test_metrics['f1']))
    torch.save(encoder, "encoder_calls.model")
avg_tuple = [sum(y) / len(y) for y in zip(*kfold_results)]
print("Overall accuracy={} Overall F1 score={}".format(avg_tuple[0], avg_tuple[1]))
